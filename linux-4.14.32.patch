diff --git a/Documentation/admin-guide/kernel-parameters.txt b/Documentation/admin-guide/kernel-parameters.txt
index fb385af4..292a3ecd 100644
--- a/Documentation/admin-guide/kernel-parameters.txt
+++ b/Documentation/admin-guide/kernel-parameters.txt
@@ -4068,6 +4068,12 @@
 
 	tdfx=		[HW,DRM]
 
+	tempesta_dbmem=	[KNL]
+			Order of 2MB memory blocks reserved on each NUMA node
+			for Tempesta database. Huge pages are used if
+			possible. Minimum value to start Tempesta is 4 (32MB).
+			Default is 8, i.e. 512MB is reserved.
+
 	test_suspend=	[SUSPEND][,N]
 			Specify "mem" (for Suspend-to-RAM) or "standby" (for
 			standby suspend) or "freeze" (for suspend type freeze)
diff --git a/arch/x86/include/asm/fpu/api.h b/arch/x86/include/asm/fpu/api.h
index a9caac9d..853777db 100644
--- a/arch/x86/include/asm/fpu/api.h
+++ b/arch/x86/include/asm/fpu/api.h
@@ -21,6 +21,10 @@
  * All other cases use kernel_fpu_begin/end() which disable preemption
  * during kernel FPU usage.
  */
+#ifdef CONFIG_SECURITY_TEMPESTA
+extern void __kernel_fpu_begin_bh(void);
+extern void __kernel_fpu_end_bh(void);
+#endif
 extern void __kernel_fpu_begin(void);
 extern void __kernel_fpu_end(void);
 extern void kernel_fpu_begin(void);
diff --git a/arch/x86/include/asm/string_64.h b/arch/x86/include/asm/string_64.h
index 533f74c3..054fe7d4 100644
--- a/arch/x86/include/asm/string_64.h
+++ b/arch/x86/include/asm/string_64.h
@@ -31,6 +31,11 @@ static __always_inline void *__inline_memcpy(void *to, const void *from, size_t
 #define __HAVE_ARCH_MEMCPY 1
 extern void *memcpy(void *to, const void *from, size_t len);
 extern void *__memcpy(void *to, const void *from, size_t len);
+#ifdef CONFIG_SECURITY_TEMPESTA
+extern void __memcpy_avx(void *to, const void *from, size_t len);
+extern int __memcmp_avx(const void *a, const void *b, size_t len);
+extern void __bzero_avx(void *s, size_t len);
+#endif
 
 #ifndef CONFIG_FORTIFY_SOURCE
 #if (__GNUC__ == 4 && __GNUC_MINOR__ < 3) || __GNUC__ < 4
diff --git a/arch/x86/kernel/fpu/core.c b/arch/x86/kernel/fpu/core.c
index f92a6593..0f6649fd 100644
--- a/arch/x86/kernel/fpu/core.c
+++ b/arch/x86/kernel/fpu/core.c
@@ -92,7 +92,8 @@ bool irq_fpu_usable(void)
 }
 EXPORT_SYMBOL(irq_fpu_usable);
 
-void __kernel_fpu_begin(void)
+void
+__kernel_fpu_begin_bh(void)
 {
 	struct fpu *fpu = &current->thread.fpu;
 
@@ -110,9 +111,10 @@ void __kernel_fpu_begin(void)
 		__cpu_invalidate_fpregs_state();
 	}
 }
-EXPORT_SYMBOL(__kernel_fpu_begin);
+EXPORT_SYMBOL(__kernel_fpu_begin_bh);
 
-void __kernel_fpu_end(void)
+void
+__kernel_fpu_end_bh(void)
 {
 	struct fpu *fpu = &current->thread.fpu;
 
@@ -121,10 +123,38 @@ void __kernel_fpu_end(void)
 
 	kernel_fpu_enable();
 }
+EXPORT_SYMBOL(__kernel_fpu_end_bh);
+
+
+void __kernel_fpu_begin(void)
+{
+#ifdef CONFIG_SECURITY_TEMPESTA
+	if (in_serving_softirq())
+		return;
+#endif
+	__kernel_fpu_begin_bh();
+}
+EXPORT_SYMBOL(__kernel_fpu_begin);
+
+void __kernel_fpu_end(void)
+{
+#ifdef CONFIG_SECURITY_TEMPESTA
+	if (in_serving_softirq())
+		return;
+#endif
+	__kernel_fpu_end_bh();
+}
 EXPORT_SYMBOL(__kernel_fpu_end);
 
+/*
+ * We don't know in which context the two functions at the below will be called,
+ * but we know preciseely that softirq uses FPU, so we have to disable softirq
+ * as well as task preemption.
+ */
+
 void kernel_fpu_begin(void)
 {
+	local_bh_disable();
 	preempt_disable();
 	__kernel_fpu_begin();
 }
@@ -134,6 +164,7 @@ void kernel_fpu_end(void)
 {
 	__kernel_fpu_end();
 	preempt_enable();
+	local_bh_enable();
 }
 EXPORT_SYMBOL_GPL(kernel_fpu_end);
 
diff --git a/arch/x86/lib/Makefile b/arch/x86/lib/Makefile
index d435c898..d2b8c11a 100644
--- a/arch/x86/lib/Makefile
+++ b/arch/x86/lib/Makefile
@@ -45,6 +45,9 @@ else
         lib-y += csum-partial_64.o csum-copy_64.o csum-wrappers_64.o
         lib-y += clear_page_64.o copy_page_64.o
         lib-y += memmove_64.o memset_64.o
+ifeq ($(CONFIG_SECURITY_TEMPESTA),y)
+	lib-y += memcmp_64.o
+endif
         lib-y += copy_user_64.o
 	lib-y += cmpxchg16b_emu.o
 endif
diff --git a/arch/x86/lib/memcmp_64.S b/arch/x86/lib/memcmp_64.S
new file mode 100644
index 00000000..0b01704a
--- /dev/null
+++ b/arch/x86/lib/memcmp_64.S
@@ -0,0 +1,152 @@
+/**
+ * AVX implementation of memcmp() returning only true/false.
+ *
+ * Copyright (C) 2018 Tempesta Technologies, Inc.
+ *
+ * This file is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU Lesser General Public License as published
+ * by the Free Software Foundation; either version 3, or (at your option)
+ * any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU Lesser General Public License for more details.
+ * See http://www.gnu.org/licenses/lgpl.html .
+ */
+#include <linux/linkage.h>
+#include <asm/alternative-asm.h>
+#include <asm/export.h>
+
+ENTRY(__memcmp_avx)
+	leaq	(%rdi,%rdx), %rax
+	leaq	128(%rdi), %rcx
+	cmpq	%rcx, %rax
+	jnb	.L128cmp_loop
+	movq	%rdi, %rcx
+.L64tail:
+	testb	$64, %dl
+	jne	.L64cmp
+.L32tail:
+	testb	$32, %dl
+	jne	.L32cmp
+.L16tail:
+	testb	$16, %dl
+	jne	.L16cmp
+.L8tail:
+	testb	$8, %dl
+	jne	.L8cmp
+.L4tail:
+	testb	$4, %dl
+	jne	.L4cmp
+.L2tail:
+	testb	$2, %dl
+	jne	.L2cmp
+.L1tail:
+	xorl	%eax, %eax
+	andl	$1, %edx
+	jne	.L1cmp
+	ret
+	.p2align 4
+.L128cmp:
+	vlddqu	-96(%rcx), %ymm0
+	vlddqu	32(%rsi), %ymm1
+	vpcmpeqd %ymm1, %ymm0, %ymm0
+	vpmovmskb %ymm0, %edi
+	cmpl	$-1, %edi
+	jne	.Lret_neq
+	vlddqu	-64(%rcx), %ymm0
+	vlddqu	64(%rsi), %ymm1
+	vpcmpeqd %ymm1, %ymm0, %ymm0
+	vpmovmskb %ymm0, %edi
+	cmpl	$-1, %edi
+	jne	.Lret_neq
+	vlddqu	-32(%rcx), %ymm0
+	vlddqu	96(%rsi), %ymm1
+	vpcmpeqd %ymm1, %ymm0, %ymm0
+	vpmovmskb %ymm0, %edi
+	addl	$1, %edi
+	jne	.Lret_neq
+	leaq	128(%rcx), %rdi
+	subq	$-128, %rsi
+	cmpq	%rdi, %rax
+	jb	.L64tail
+	movq	%rdi, %rcx
+.L128cmp_loop:
+	vlddqu	-128(%rcx), %ymm0
+	vlddqu	(%rsi), %ymm1
+	vpcmpeqd %ymm1, %ymm0, %ymm0
+	vpmovmskb %ymm0, %edi
+	cmpl	$-1, %edi
+	je	.L128cmp
+.Lret_neq:
+	movl	$1, %eax
+	ret
+.L64cmp:
+	vlddqu	(%rcx), %ymm0
+	vlddqu	(%rsi), %ymm1
+	vpcmpeqd %ymm1, %ymm0, %ymm0
+	vpmovmskb %ymm0, %eax
+	cmpl	$-1, %eax
+	jne	.Lret_neq
+	vlddqu	32(%rcx), %ymm0
+	vlddqu	32(%rsi), %ymm1
+	vpcmpeqd %ymm1, %ymm0, %ymm0
+	vpmovmskb %ymm0, %eax
+	cmpl	$-1, %eax
+	jne	.Lret_neq
+	addq	$64, %rcx
+	addq	$64, %rsi
+	jmp	.L32tail
+.L32cmp:
+	vlddqu	(%rcx), %ymm0
+	vlddqu	(%rsi), %ymm1
+	movl	$1, %eax
+	vpcmpeqd %ymm1, %ymm0, %ymm0
+	vpmovmskb %ymm0, %edi
+	cmpl	$-1, %edi
+	jne	.Lret
+	addq	$32, %rcx
+	addq	$32, %rsi
+	jmp	.L16tail
+.L16cmp:
+	vlddqu	(%rcx), %xmm0
+	vlddqu	(%rsi), %xmm1
+	movl	$1, %eax
+	vpcmpeqw %xmm1, %xmm0, %xmm0
+	vpmovmskb %xmm0, %edi
+	cmpl	$65535, %edi
+	jne	.Lret
+	addq	$16, %rcx
+	addq	$16, %rsi
+	jmp	.L8tail
+.L8cmp:
+	movq	(%rsi), %rax
+	cmpq	%rax, (%rcx)
+	jne	.Lret_neq
+	addq	$8, %rcx
+	addq	$8, %rsi
+	jmp	.L4tail
+.L4cmp:
+	movl	(%rsi), %eax
+	cmpl	%eax, (%rcx)
+	jne	.Lret_neq
+	addq	$4, %rcx
+	addq	$4, %rsi
+	jmp	.L2tail
+.L2cmp:
+	movzwl	(%rsi), %eax
+	cmpw	%ax, (%rcx)
+	jne	.Lret_neq
+	addq	$2, %rcx
+	addq	$2, %rsi
+	jmp	.L1tail
+.L1cmp:
+	movzbl	(%rsi), %eax
+	cmpb	%al, (%rcx)
+	setne	%al
+	movzbl	%al, %eax
+.Lret:
+	ret
+ENDPROC(__memcmp_avx)
+EXPORT_SYMBOL(__memcmp_avx)
diff --git a/arch/x86/lib/memcpy_64.S b/arch/x86/lib/memcpy_64.S
index 9a53a06e..80f10a33 100644
--- a/arch/x86/lib/memcpy_64.S
+++ b/arch/x86/lib/memcpy_64.S
@@ -182,6 +182,116 @@ ENTRY(memcpy_orig)
 	retq
 ENDPROC(memcpy_orig)
 
+#ifdef CONFIG_SECURITY_TEMPESTA
+ENTRY(__memcpy_avx)
+	movq	%rdx, %rax
+	leaq	128(%rsi), %rcx
+	movq	%rsi, %r8
+	andq	$-32, %rax
+	addq	%rsi, %rax
+	cmpq	%rcx, %rax
+	jnb	.L128cpy
+	testb	$64, %dl
+	jne	.L64cpy
+.L32tail:
+	testb	$32, %dl
+	jne	.L32cpy
+.L16tail:
+	movq	%r8, %rcx
+	movq	%rdi, %rax
+	testb	$16, %dl
+	jne	.L16cpy
+.L8tail:
+	testb	$8, %dl
+	jne	.L8cpy
+.L4tail:
+	testb	$4, %dl
+	jne	.L4cpy
+.L2tail:
+	testb	$2, %dl
+	jne	.L2cpy
+.L1tail:
+	andl	$1, %edx
+	jne	.L1cpy
+	/* Don't clean the registers w/ vzeroupper. */
+	ret
+	.p2align 4
+.Lrepeat128cpy:
+	movq	%r8, %rsi
+.L128cpy:
+	vlddqu	(%rsi), %ymm3
+	vlddqu	32(%rsi), %ymm2
+	movq	%rcx, %r8
+	leaq	96(%rdi), %rcx
+	vlddqu	64(%rsi), %ymm1
+	vlddqu	96(%rsi), %ymm0
+	subq	$-128, %rdi
+	vmovdqu	%ymm3, -128(%rdi)
+	vmovdqu	%ymm2, -96(%rdi)
+	vmovdqu	%ymm1, -64(%rdi)
+	vmovdqu	%ymm0, (%rcx)
+	leaq	256(%rsi), %rcx
+	cmpq	%rcx, %rax
+	jnb	.Lrepeat128cpy
+	testb	$64, %dl
+	je	.L32tail
+.L64cpy:
+	leaq	32(%r8), %rax
+	vlddqu	(%r8), %ymm1
+	vlddqu	(%rax), %ymm0
+	leaq	32(%rdi), %rax
+	addq	$64, %r8
+	addq	$64, %rdi
+	vmovdqu	%ymm1, -64(%rdi)
+	vmovdqu	%ymm0, (%rax)
+	testb	$32, %dl
+	je	.L16tail
+.L32cpy:
+	vlddqu	(%r8), %ymm0
+	addq	$32, %rdi
+	addq	$32, %r8
+	movq	%r8, %rcx
+	vmovdqu	%ymm0, -32(%rdi)
+	movq	%rdi, %rax
+	testb	$16, %dl
+	je	.L8tail
+.L16cpy:
+	vlddqu	(%r8), %xmm0
+	addq	$16, %rcx
+	addq	$16, %rax
+	vmovups	%xmm0, (%rdi)
+	testb	$8, %dl
+	je	.L4tail
+.L8cpy:
+	movq	(%rcx), %rsi
+	addq	$8, %rax
+	addq	$8, %rcx
+	movq	%rsi, -8(%rax)
+	testb	$4, %dl
+	je	.L2tail
+.L4cpy:
+	movl	(%rcx), %esi
+	addq	$4, %rax
+	addq	$4, %rcx
+	movl	%esi, -4(%rax)
+	testb	$2, %dl
+	je	.L1tail
+.L2cpy:
+	movzwl	(%rcx), %esi
+	addq	$2, %rax
+	addq	$2, %rcx
+	movw	%si, -2(%rax)
+	andl	$1, %edx
+	je	.Lret
+.L1cpy:
+	movzbl	(%rcx), %edx
+	movb	%dl, (%rax)
+.Lret:
+	ret
+ENDPROC(__memcpy_avx)
+EXPORT_SYMBOL(__memcpy_avx)
+#endif
+
 #ifndef CONFIG_UML
 /*
  * memcpy_mcsafe_unrolled - memory copy with machine check exception handling
diff --git a/arch/x86/lib/memset_64.S b/arch/x86/lib/memset_64.S
index 9bc861c7..78f9c04d 100644
--- a/arch/x86/lib/memset_64.S
+++ b/arch/x86/lib/memset_64.S
@@ -140,3 +140,91 @@ ENTRY(memset_orig)
 	jmp .Lafter_bad_alignment
 .Lfinal:
 ENDPROC(memset_orig)
+
+#ifdef CONFIG_SECURITY_TEMPESTA
+ENTRY(__bzero_avx)
+	movq	%rsi, %rax
+	movq	%rdi, %rdx
+	andq	$-128, %rax
+	addq	%rdi, %rax
+	cmpq	%rax, %rdi
+	vpxor	%ymm0, %ymm0, %ymm0
+	jb	.L128zer
+	testb	$64, %sil
+	jne	.L64zer
+.L32tail:
+	testb	$32, %sil
+	jne	.L32zer
+.L16tail:
+	movq	%rdx, %rax
+	testb	$16, %sil
+	jne	.L16zer
+.L8tail:
+	testb	$8, %sil
+	jne	.L8zer
+.L4tail:
+	testb	$4, %sil
+	jne	.L4zer
+.L2tail:
+	testb	$2, %sil
+	jne	.L2zer
+.L1tail:
+	andl	$1, %esi
+	jne	.L1zer
+	/* Don't clean the registers w/ vzeroupper. */
+	ret
+	.p2align 4
+.L128zer:
+	vmovdqu	%ymm0, (%rdx)
+	subq	$-128, %rdx
+	vmovdqu	%ymm0, -96(%rdx)
+	vmovdqu	%ymm0, -64(%rdx)
+	vmovdqu	%ymm0, -32(%rdx)
+	cmpq	%rdx, %rax
+	ja	.L128zer
+	movq	%rdi, %rdx
+	notq	%rdx
+	addq	%rdx, %rax
+	andq	$-128, %rax
+	leaq	128(%rdi,%rax), %rdx
+	testb	$64, %sil
+	je	.L32tail
+.L64zer:
+	addq	$64, %rdx
+	vmovdqu	%ymm0, -64(%rdx)
+	vmovdqu	%ymm0, -32(%rdx)
+	testb	$32, %sil
+	je	.L16tail
+.L32zer:
+	addq	$32, %rdx
+	vmovdqu	%ymm0, -32(%rdx)
+	movq	%rdx, %rax
+	testb	$16, %sil
+	je	.L8tail
+.L16zer:
+	addq	$16, %rax
+	vmovups	%xmm0, (%rdx)
+	testb	$8, %sil
+	je	.L4tail
+.L8zer:
+	movq	$0, (%rax)
+	addq	$8, %rax
+	testb	$4, %sil
+	je	.L2tail
+.L4zer:
+	movl	$0, (%rax)
+	addq	$4, %rax
+	testb	$2, %sil
+	je	.L1tail
+.L2zer:
+	movw	$0, (%rax)
+	addq	$2, %rax
+	andl	$1, %esi
+	je	.Lret
+.L1zer:
+	movb	$0, (%rax)
+.Lret:
+	ret
+ENDPROC(__bzero_avx)
+EXPORT_SYMBOL(__bzero_avx)
+#endif
diff --git a/include/linux/interrupt.h b/include/linux/interrupt.h
index 69c23821..58486beb 100644
--- a/include/linux/interrupt.h
+++ b/include/linux/interrupt.h
@@ -454,13 +454,13 @@ extern bool force_irqthreads;
    tasklets are more than enough. F.e. all serial device BHs et
    al. should be converted to tasklets, not to softirqs.
  */
-
+/* Tempesta: process RX before TX to proxy traffic in one softirq shot. */
 enum
 {
 	HI_SOFTIRQ=0,
 	TIMER_SOFTIRQ,
-	NET_TX_SOFTIRQ,
 	NET_RX_SOFTIRQ,
+	NET_TX_SOFTIRQ,
 	BLOCK_SOFTIRQ,
 	IRQ_POLL_SOFTIRQ,
 	TASKLET_SOFTIRQ,
@@ -505,7 +505,7 @@ extern void softirq_init(void);
 extern void __raise_softirq_irqoff(unsigned int nr);
 
 extern void raise_softirq_irqoff(unsigned int nr);
-extern void raise_softirq(unsigned int nr);
+void raise_softirq(unsigned int nr);
 
 DECLARE_PER_CPU(struct task_struct *, ksoftirqd);
 
diff --git a/include/linux/net.h b/include/linux/net.h
index caeb159a..274c6cc0 100644
--- a/include/linux/net.h
+++ b/include/linux/net.h
@@ -209,6 +209,8 @@ struct net_proto_family {
 	struct module	*owner;
 };
 
+extern const struct net_proto_family *get_proto_family(int family);
+
 struct iovec;
 struct kvec;
 
diff --git a/include/linux/skbuff.h b/include/linux/skbuff.h
index be45224b..f50c4a8e 100644
--- a/include/linux/skbuff.h
+++ b/include/linux/skbuff.h
@@ -722,7 +722,11 @@ struct sk_buff {
 				peeked:1,
 				head_frag:1,
 				xmit_more:1,
+#ifdef CONFIG_SECURITY_TEMPESTA
+				skb_page:1;
+#else
 				__unused:1; /* one bit hole */
+#endif
 
 	/* fields enclosed in headers_start/headers_end are copied
 	 * using a single memcpy() in __copy_skb_header()
@@ -777,6 +781,9 @@ struct sk_buff {
 	__u8			tc_redirected:1;
 	__u8			tc_from_ingress:1;
 #endif
+#ifdef CONFIG_SECURITY_TEMPESTA
+	__u8			tail_lock:1;
+#endif
 
 #ifdef CONFIG_NET_SCHED
 	__u16			tc_index;	/* traffic control index */
@@ -842,6 +849,46 @@ struct sk_buff {
  */
 #include <linux/slab.h>
 
+#ifdef CONFIG_SECURITY_TEMPESTA
+/*
+ * We can't use SIMD without FPU state saving, which is expensive so just fall
+ * back to plain memcpy(), memset() and memcmp() correspondingly.
+ */
+static inline void
+memcpy_fast(void *to, const void *from, size_t len)
+{
+	if (unlikely(!in_serving_softirq())) {
+		memcpy(to, from, len);
+		return;
+	}
+
+	__memcpy_avx(to, from, len);
+}
+
+static inline int
+memcmp_fast(const void *a, const void *b, size_t len)
+{
+	if (unlikely(!in_serving_softirq()))
+		return memcmp(a, b, len);
+
+	return __memcmp_avx(a, b, len);
+}
+
+static inline void
+bzero_fast(void *s, size_t len)
+{
+	if (unlikely(!in_serving_softirq())) {
+		memset(s, 0, len);
+		return;
+	}
+
+	__bzero_avx(s, len);
+}
+#else
+#define memcpy_fast(to, from, len)	memcpy(to, from, len)
+#define memcmp_fast(a, b, len)		memcmp(a, b, len)
+#define bzero_fast(s, len)		memset(s, 0, len)
+#endif
 
 #define SKB_ALLOC_FCLONE	0x01
 #define SKB_ALLOC_RX		0x02
@@ -1188,7 +1235,7 @@ static inline bool skb_flow_dissect_flow_keys(const struct sk_buff *skb,
 					      struct flow_keys *flow,
 					      unsigned int flags)
 {
-	memset(flow, 0, sizeof(*flow));
+	bzero_fast(flow, sizeof(*flow));
 	return __skb_flow_dissect(skb, &flow_keys_dissector, flow,
 				  NULL, 0, 0, 0, flags);
 }
@@ -1198,7 +1245,7 @@ static inline bool skb_flow_dissect_flow_keys_buf(struct flow_keys *flow,
 						  int nhoff, int hlen,
 						  unsigned int flags)
 {
-	memset(flow, 0, sizeof(*flow));
+	bzero_fast(flow, sizeof(*flow));
 	return __skb_flow_dissect(NULL, &flow_keys_buf_dissector, flow,
 				  data, proto, nhoff, hlen, flags);
 }
@@ -1885,7 +1932,11 @@ static inline struct sk_buff *__skb_dequeue_tail(struct sk_buff_head *list)
 
 static inline bool skb_is_nonlinear(const struct sk_buff *skb)
 {
+#ifdef CONFIG_SECURITY_TEMPESTA
+	return skb->tail_lock || skb->data_len;
+#else
 	return skb->data_len;
+#endif
 }
 
 static inline unsigned int skb_headlen(const struct sk_buff *skb)
@@ -2023,7 +2074,7 @@ static inline void *__skb_put_zero(struct sk_buff *skb, unsigned int len)
 {
 	void *tmp = __skb_put(skb, len);
 
-	memset(tmp, 0, len);
+	bzero_fast(tmp, len);
 	return tmp;
 }
 
@@ -2032,7 +2083,7 @@ static inline void *__skb_put_data(struct sk_buff *skb, const void *data,
 {
 	void *tmp = __skb_put(skb, len);
 
-	memcpy(tmp, data, len);
+	memcpy_fast(tmp, data, len);
 	return tmp;
 }
 
@@ -2045,7 +2096,7 @@ static inline void *skb_put_zero(struct sk_buff *skb, unsigned int len)
 {
 	void *tmp = skb_put(skb, len);
 
-	memset(tmp, 0, len);
+	bzero_fast(tmp, len);
 
 	return tmp;
 }
@@ -2055,7 +2106,7 @@ static inline void *skb_put_data(struct sk_buff *skb, const void *data,
 {
 	void *tmp = skb_put(skb, len);
 
-	memcpy(tmp, data, len);
+	memcpy_fast(tmp, data, len);
 
 	return tmp;
 }
@@ -2124,6 +2175,20 @@ static inline unsigned int skb_headroom(const struct sk_buff *skb)
 	return skb->data - skb->head;
 }
 
+#ifdef CONFIG_SECURITY_TEMPESTA
+/**
+ *	skb_tailroom_locked - bytes at buffer end
+ *	@skb: buffer to check
+ *
+ *	Return the number of bytes of free space at the tail of an sk_buff with
+ *	respect to tail locking only.
+ */
+static inline int skb_tailroom_locked(const struct sk_buff *skb)
+{
+	return skb->tail_lock ? 0 : skb->end - skb->tail;
+}
+#endif
+
 /**
  *	skb_tailroom - bytes at buffer end
  *	@skb: buffer to check
@@ -3350,21 +3415,21 @@ static inline void skb_copy_from_linear_data(const struct sk_buff *skb,
 					     void *to,
 					     const unsigned int len)
 {
-	memcpy(to, skb->data, len);
+	memcpy_fast(to, skb->data, len);
 }
 
 static inline void skb_copy_from_linear_data_offset(const struct sk_buff *skb,
 						    const int offset, void *to,
 						    const unsigned int len)
 {
-	memcpy(to, skb->data + offset, len);
+	memcpy_fast(to, skb->data + offset, len);
 }
 
 static inline void skb_copy_to_linear_data(struct sk_buff *skb,
 					   const void *from,
 					   const unsigned int len)
 {
-	memcpy(skb->data, from, len);
+	memcpy_fast(skb->data, from, len);
 }
 
 static inline void skb_copy_to_linear_data_offset(struct sk_buff *skb,
@@ -3372,7 +3437,7 @@ static inline void skb_copy_to_linear_data_offset(struct sk_buff *skb,
 						  const void *from,
 						  const unsigned int len)
 {
-	memcpy(skb->data + offset, from, len);
+	memcpy_fast(skb->data + offset, from, len);
 }
 
 void skb_init(void);
diff --git a/include/linux/tempesta.h b/include/linux/tempesta.h
new file mode 100644
index 00000000..55049bd3
--- /dev/null
+++ b/include/linux/tempesta.h
@@ -0,0 +1,54 @@
+/**
+ * Linux interface for Tempesta FW.
+ *
+ * Copyright (C) 2014 NatSys Lab. (info@natsys-lab.com).
+ * Copyright (C) 2015-2016 Tempesta Technologies, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License,
+ * or (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.
+ * See the GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License along with
+ * this program; if not, write to the Free Software Foundation, Inc., 59
+ * Temple Place - Suite 330, Boston, MA 02111-1307, USA.
+ */
+#ifndef __TEMPESTA_H__
+#define __TEMPESTA_H__
+
+#include <net/sock.h>
+
+typedef void (*TempestaTxAction)(void);
+
+typedef struct {
+	int (*sk_alloc)(struct sock *sk);
+	void (*sk_free)(struct sock *sk);
+	int (*sock_tcp_rcv)(struct sock *sk, struct sk_buff *skb);
+} TempestaOps;
+
+typedef struct {
+	unsigned long	addr;
+	unsigned long	pages; /* number of 4KB pages */
+} TempestaMapping;
+
+/* Security hooks. */
+int tempesta_new_clntsk(struct sock *newsk);
+void tempesta_register_ops(TempestaOps *tops);
+void tempesta_unregister_ops(TempestaOps *tops);
+
+/* Network hooks. */
+void tempesta_set_tx_action(TempestaTxAction action);
+void tempesta_del_tx_action(void);
+
+/* Memory management. */
+void tempesta_reserve_pages(void);
+void tempesta_reserve_vmpages(void);
+int tempesta_get_mapping(int node, TempestaMapping **tm);
+
+#endif /* __TEMPESTA_H__ */
+
diff --git a/include/net/sock.h b/include/net/sock.h
index 9bd5d680..ece2126c 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -771,6 +771,9 @@ enum sock_flags {
 	SOCK_FILTER_LOCKED, /* Filter cannot be changed anymore */
 	SOCK_SELECT_ERR_QUEUE, /* Wake select on error queue */
 	SOCK_RCU_FREE, /* wait rcu grace period in sk_destruct() */
+#ifdef CONFIG_SECURITY_TEMPESTA
+	SOCK_TEMPESTA, /* The socket is managed by Tempesta FW */
+#endif
 };
 
 #define SK_FLAGS_TIMESTAMP ((1UL << SOCK_TIMESTAMP) | (1UL << SOCK_TIMESTAMPING_RX_SOFTWARE))
@@ -1737,8 +1740,7 @@ static inline void sk_rethink_txhash(struct sock *sk)
 static inline struct dst_entry *
 __sk_dst_get(struct sock *sk)
 {
-	return rcu_dereference_check(sk->sk_dst_cache,
-				     lockdep_sock_is_held(sk));
+	return rcu_dereference_raw(sk->sk_dst_cache);
 }
 
 static inline struct dst_entry *
diff --git a/include/net/tcp.h b/include/net/tcp.h
index d323d4fa..b2a6ca58 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -333,6 +333,7 @@ bool tcp_check_oom(struct sock *sk, int shift);
 
 
 extern struct proto tcp_prot;
+extern struct proto tcpv6_prot;
 
 #define TCP_INC_STATS(net, field)	SNMP_INC_STATS((net)->mib.tcp_statistics, field)
 #define __TCP_INC_STATS(net, field)	__SNMP_INC_STATS((net)->mib.tcp_statistics, field)
@@ -611,6 +612,17 @@ static inline int tcp_bound_to_half_wnd(struct tcp_sock *tp, int pktsize)
 /* tcp.c */
 void tcp_get_info(struct sock *, struct tcp_info *);
 
+/* Routines required by Tempesta FW. */
+void tcp_cleanup_rbuf(struct sock *sk, int copied);
+extern void tcp_push(struct sock *sk, int flags, int mss_now, int nonagle,
+		     int size_goal);
+extern int tcp_send_mss(struct sock *sk, int *size_goal, int flags);
+extern void tcp_mark_push(struct tcp_sock *tp, struct sk_buff *skb);
+extern void tcp_init_nondata_skb(struct sk_buff *skb, u32 seq, u8 flags);
+extern void tcp_queue_skb(struct sock *sk, struct sk_buff *skb);
+extern int tcp_close_state(struct sock *sk);
+extern void skb_entail(struct sock *sk, struct sk_buff *skb);
+
 /* Read 'sendfile()'-style from a TCP socket */
 int tcp_read_sock(struct sock *sk, read_descriptor_t *desc,
 		  sk_read_actor_t recv_actor);
diff --git a/init/main.c b/init/main.c
index 2d355a61..0d40204c 100644
--- a/init/main.c
+++ b/init/main.c
@@ -95,6 +95,8 @@
 #include <asm/sections.h>
 #include <asm/cacheflush.h>
 
+#include <linux/tempesta.h>
+
 static int kernel_init(void *);
 
 extern void init_IRQ(void);
@@ -500,6 +502,15 @@ static void __init mm_init(void)
 	 */
 	page_ext_init_flatmem();
 	mem_init();
+
+#ifdef CONFIG_SECURITY_TEMPESTA
+	/*
+	 * Tempesta: reserve pages just when zones are initialized
+	 * to get continous address space of huge pages.
+	 */
+	tempesta_reserve_pages();
+#endif
+
 	kmem_cache_init();
 	pgtable_init();
 	vmalloc_init();
@@ -508,6 +519,11 @@ static void __init mm_init(void)
 	init_espfix_bsp();
 	/* Should be run after espfix64 is set up. */
 	pti_init();
+
+#ifdef CONFIG_SECURITY_TEMPESTA
+	/* Try vmalloc() if the previous one failed. */
+	tempesta_reserve_vmpages();
+#endif
 }
 
 asmlinkage __visible void __init start_kernel(void)
diff --git a/kernel/softirq.c b/kernel/softirq.c
index e89c3b0c..7715cad7 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -26,6 +26,7 @@
 #include <linux/smpboot.h>
 #include <linux/tick.h>
 #include <linux/irq.h>
+#include <asm/fpu/api.h>
 
 #define CREATE_TRACE_POINTS
 #include <trace/events/irq.h>
@@ -261,6 +262,10 @@ asmlinkage __visible void __softirq_entry __do_softirq(void)
 	__local_bh_disable_ip(_RET_IP_, SOFTIRQ_OFFSET);
 	in_hardirq = lockdep_softirq_start();
 
+#ifdef CONFIG_SECURITY_TEMPESTA
+	__kernel_fpu_begin_bh();
+#endif
+
 restart:
 	/* Reset the pending bitmask before enabling irqs */
 	set_softirq_pending(0);
@@ -305,6 +310,9 @@ asmlinkage __visible void __softirq_entry __do_softirq(void)
 		wakeup_softirqd();
 	}
 
+#ifdef CONFIG_SECURITY_TEMPESTA
+	__kernel_fpu_end_bh();
+#endif
 	lockdep_softirq_end(in_hardirq);
 	account_irq_exit_time(current);
 	__local_bh_enable(SOFTIRQ_OFFSET);
@@ -437,6 +445,7 @@ void raise_softirq(unsigned int nr)
 	raise_softirq_irqoff(nr);
 	local_irq_restore(flags);
 }
+EXPORT_SYMBOL(raise_softirq);
 
 void __raise_softirq_irqoff(unsigned int nr)
 {
diff --git a/mm/Makefile b/mm/Makefile
index e7ebd176..9f4a870c 100644
--- a/mm/Makefile
+++ b/mm/Makefile
@@ -104,3 +104,4 @@ obj-$(CONFIG_DEBUG_PAGE_REF) += debug_page_ref.o
 obj-$(CONFIG_HARDENED_USERCOPY) += usercopy.o
 obj-$(CONFIG_PERCPU_STATS) += percpu-stats.o
 obj-$(CONFIG_HMM) += hmm.o
+obj-$(CONFIG_SECURITY_TEMPESTA) += tempesta_mm.o
diff --git a/mm/tempesta_mm.c b/mm/tempesta_mm.c
new file mode 100644
index 00000000..8f7bc5f4
--- /dev/null
+++ b/mm/tempesta_mm.c
@@ -0,0 +1,278 @@
+/**
+ *		Tempesta Memory Reservation
+ *
+ * Copyright (C) 2015-2018 Tempesta Technologies, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License,
+ * or (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
+ * FOR A PARTICULAR PURPOSE.
+ * See the GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License along with
+ * this program; if not, write to the Free Software Foundation, Inc., 59
+ * Temple Place - Suite 330, Boston, MA 02111-1307, USA.
+ */
+#include <linux/gfp.h>
+#include <linux/hugetlb.h>
+#include <linux/tempesta.h>
+#include <linux/topology.h>
+#include <linux/vmalloc.h>
+
+#include "internal.h"
+
+#define MAX_PGORDER		16	/* 128GB per one table */
+#define MIN_PGORDER		4	/* 32MB */
+#define DEFAULT_PGORDER		8	/* 512MB */
+/* Modern processors support up to 1.5TB of RAM, be ready for 2TB. */
+#define GREEDY_ARNUM		(1024 * 1024 + 1)
+#define PGNUM			(1 << pgorder)
+#define PGNUM4K			(PGNUM * (1 << HUGETLB_PAGE_ORDER))
+
+static int pgorder = DEFAULT_PGORDER;
+static gfp_t gfp_f = GFP_HIGHUSER | __GFP_COMP | __GFP_THISNODE | __GFP_ZERO
+		     | __GFP_RETRY_MAYFAIL;
+static TempestaMapping map[MAX_NUMNODES];
+/*
+ * Modern x86-64 has not more than 512GB RAM per physical node.
+ * This is very large amount of memory, but it will be freed when
+ * initialization phase ends.
+ */
+static struct page *greedy[GREEDY_ARNUM] __initdata = { 0 };
+
+static int __init
+tempesta_setup_pages(char *str)
+{
+	get_option(&str, &pgorder);
+	if (pgorder < MIN_PGORDER) {
+		pr_err("Tempesta: bad dbmem value %d, must be [%d:%d]\n",
+		       pgorder, MIN_PGORDER, MAX_PGORDER);
+		pgorder = MIN_PGORDER;
+	}
+	if (pgorder > MAX_PGORDER) {
+		pr_err("Tempesta: bad dbmem value %d, must be [%d:%d]\n",
+		       pgorder, MIN_PGORDER, MAX_PGORDER);
+		pgorder = MAX_PGORDER;
+	}
+
+	return 1;
+}
+__setup("tempesta_dbmem=", tempesta_setup_pages);
+
+/**
+ * The code is somewhat stollen from mm/hugetlb.c.
+ */
+static struct page *
+tempesta_alloc_hpage(int nid)
+{
+	struct page *p;
+
+	p = alloc_pages_node(nid, gfp_f, HUGETLB_PAGE_ORDER);
+	if (!p)
+		return NULL;
+
+	count_vm_event(HTLB_BUDDY_PGALLOC);
+
+	__ClearPageReserved(p);
+	prep_compound_page(p, HUGETLB_PAGE_ORDER);
+
+	/* Acquire the page immediately. */
+	set_page_refcounted(p);
+
+	return p;
+}
+
+static void
+tempesta_free_hpage(struct page *p)
+{
+	__free_pages(p, HUGETLB_PAGE_ORDER);
+}
+
+/**
+ * Greedely alloc huge pages and try to find continous region organized
+ * by sorted set of allocated pages. When the region is found, all pages
+ * out of it are returned to system.
+ */
+static struct page *
+tempesta_alloc_contmem(int nid)
+{
+	long min = -1, start = -1, curr = 0, end = -1, max = -1;
+	struct page *p;
+
+	while (1) {
+		p = tempesta_alloc_hpage(nid);
+		if (!p)
+			goto err;
+		curr = ((long)page_address(p) - PAGE_OFFSET) >> HPAGE_SHIFT;
+		/*
+		 * The first kernel mapped page is always reserved.
+		 * Keep untouched (zero) bounds for faster lookups.
+		 */
+		BUG_ON(curr < 1 || curr >= GREEDY_ARNUM);
+		greedy[curr] = p;
+
+		/* First time initialization. */
+		if (min < 0) {
+			min = start = end = max = curr;
+		} else {
+			/* Update bounds for faster pages return. */
+			if (min > curr)
+				min = curr;
+			if (max < curr)
+				max = curr;
+			/* Update continous memory segment bounds. */
+			if (curr == end + 1) {
+				while (end <= max && greedy[end + 1])
+					++end;
+			}
+			else if (curr + 1 == start) {
+				while (start >= min && greedy[start - 1])
+					--start;
+			}
+			else {
+				/* Try to find new continous segment. */
+				long i, d_max = 0, good_start = start = min;
+				for (i = min; i <= max; ++i) {
+					if (greedy[i]) {
+						if (start == -1)
+							start = i;
+						end = i;
+						if (i - start + 1 == PGNUM)
+							break;
+						continue;
+					}
+
+					if (start > 0 && end - start > d_max) {
+						good_start = start;
+						d_max = end - start;
+					}
+					start = -1;
+				}
+				if (end - start < d_max) {
+					start = good_start;
+					end = start + d_max;
+				}
+			}
+		}
+
+		if (end - start + 1 == PGNUM)
+			break; /* continous space is built! */
+	}
+
+	/* Return unnecessary pages. */
+	BUG_ON(min < 0 || start < 0 || end < 0 || max < 0);
+	for ( ; min < start; ++min)
+		if (greedy[min]) {
+			tempesta_free_hpage(greedy[min]);
+			greedy[min] = NULL;
+		}
+	for ( ; max > end; --max)
+		if (greedy[max]) {
+			tempesta_free_hpage(greedy[max]);
+			greedy[max] = NULL;
+		}
+	return greedy[start];
+
+err:
+	pr_err("Tempesta: cannot allocate %u continous huge pages at node"
+	       " %d\n", PGNUM, nid);
+	for ( ; min >= 0 && min <= max; ++min)
+		if (greedy[min]) {
+			tempesta_free_hpage(greedy[min]);
+			greedy[min] = NULL;
+		}
+	return NULL;
+}
+
+/**
+ * Allocate continous virtual space of huge pages for Tempesta.
+ * We do not use giantic 1GB pages since not all modern x86-64 CPUs
+ * allows them in virtualized mode.
+ *
+ * TODO try firstly to allocate giantic pages, next huge pages and finally
+ * fallback to common 4KB pages allocation if previous tries failed.
+ */
+void __init
+tempesta_reserve_pages(void)
+{
+	int nid;
+	struct page *p;
+
+	for_each_online_node(nid) {
+		p = tempesta_alloc_contmem(nid);
+		if (!p)
+			goto err;
+
+		map[nid].addr = (unsigned long)page_address(p);
+		map[nid].pages = PGNUM4K;
+
+		pr_info("Tempesta: allocated huge pages space %p %luMB at node"
+			" %d\n", page_address(p),
+			PGNUM4K * PAGE_SIZE / (1024 * 1024), nid);
+	}
+
+	return;
+err:
+	for_each_online_node(nid) {
+		struct page *pend;
+		if (!map[nid].addr)
+			continue;
+		for (p = virt_to_page(map[nid].addr), pend = p + PGNUM4K;
+		     p < pend; p += 1 << HUGETLB_PAGE_ORDER)
+			tempesta_free_hpage(p);
+	}
+	memset(map, 0, sizeof(map));
+}
+
+/**
+ * Allocates necessary space if tempesta_reserve_pages() failed.
+ */
+void __init
+tempesta_reserve_vmpages(void)
+{
+	int nid, maps = 0;
+	size_t vmsize = PGNUM * (1 << HPAGE_SHIFT);
+
+	for_each_online_node(nid)
+		maps += !!map[nid].addr;
+
+	BUG_ON(maps && maps < nr_online_nodes);
+	if (maps == nr_online_nodes)
+		return;
+
+	for_each_online_node(nid) {
+		pr_warn("Tempesta: allocate %u vmalloc pages at node %d\n",
+			PGNUM4K, nid);
+
+		map[nid].addr = (unsigned long)vzalloc_node(vmsize, nid);
+		if (!map[nid].addr)
+			goto err;
+		map[nid].pages = PGNUM4K;
+	}
+
+	return;
+err:
+	pr_err("Tempesta: cannot vmalloc area of %lu bytes at node %d\n",
+	       vmsize, nid);
+	for_each_online_node(nid)
+		if (map[nid].addr)
+			vfree((void *)map[nid].addr);
+	memset(map, 0, sizeof(map));
+}
+
+int
+tempesta_get_mapping(int nid, TempestaMapping **tm)
+{
+	if (unlikely(!map[nid].addr))
+		return -ENOMEM;
+
+	*tm = &map[nid];
+
+	return 0;
+}
+EXPORT_SYMBOL(tempesta_get_mapping);
+
diff --git a/net/core/dev.c b/net/core/dev.c
index 387af341..70d2fadd 100644
--- a/net/core/dev.c
+++ b/net/core/dev.c
@@ -4095,6 +4095,28 @@ int netif_rx_ni(struct sk_buff *skb)
 }
 EXPORT_SYMBOL(netif_rx_ni);
 
+#ifdef CONFIG_SECURITY_TEMPESTA
+#include <linux/tempesta.h>
+
+static TempestaTxAction __rcu tempesta_tx_action = NULL;
+
+void
+tempesta_set_tx_action(TempestaTxAction action)
+{
+	rcu_assign_pointer(tempesta_tx_action, action);
+}
+EXPORT_SYMBOL(tempesta_set_tx_action);
+
+void
+tempesta_del_tx_action(void)
+{
+	rcu_assign_pointer(tempesta_tx_action, NULL);
+	synchronize_rcu();
+}
+EXPORT_SYMBOL(tempesta_del_tx_action);
+#endif
+
+
 static __latent_entropy void net_tx_action(struct softirq_action *h)
 {
 	struct softnet_data *sd = this_cpu_ptr(&softnet_data);
@@ -4127,6 +4149,20 @@ static __latent_entropy void net_tx_action(struct softirq_action *h)
 		__kfree_skb_flush();
 	}
 
+#ifdef CONFIG_SECURITY_TEMPESTA
+	{
+		TempestaTxAction action;
+
+		rcu_read_lock();
+
+		action = rcu_dereference(tempesta_tx_action);
+		if (likely(action))
+			action();
+
+		rcu_read_unlock();
+	}
+#endif
+
 	if (sd->output_queue) {
 		struct Qdisc *head;
 
@@ -4726,9 +4762,9 @@ static void gro_list_prepare(struct napi_struct *napi, struct sk_buff *skb)
 			diffs |= compare_ether_header(skb_mac_header(p),
 						      skb_mac_header(skb));
 		else if (!diffs)
-			diffs = memcmp(skb_mac_header(p),
-				       skb_mac_header(skb),
-				       maclen);
+			diffs = memcmp_fast(skb_mac_header(p),
+					    skb_mac_header(skb),
+					    maclen);
 		NAPI_GRO_CB(p)->same_flow = !diffs;
 	}
 }
@@ -4758,7 +4794,7 @@ static void gro_pull_from_frag0(struct sk_buff *skb, int grow)
 
 	BUG_ON(skb->end - skb->tail < grow);
 
-	memcpy(skb_tail_pointer(skb), NAPI_GRO_CB(skb)->frag0, grow);
+	memcpy_fast(skb_tail_pointer(skb), NAPI_GRO_CB(skb)->frag0, grow);
 
 	skb->data_len -= grow;
 	skb->tail += grow;
@@ -4917,6 +4953,11 @@ static void napi_skb_free_stolen_head(struct sk_buff *skb)
 {
 	skb_dst_drop(skb);
 	secpath_reset(skb);
+#ifdef CONFIG_SECURITY_TEMPESTA
+	if (skb->skb_page)
+		put_page(virt_to_page(skb));
+	else
+#endif
 	kmem_cache_free(skbuff_head_cache, skb);
 }
 
diff --git a/net/core/request_sock.c b/net/core/request_sock.c
index 9b8727c6..a496888a 100644
--- a/net/core/request_sock.c
+++ b/net/core/request_sock.c
@@ -134,3 +134,4 @@ void reqsk_fastopen_remove(struct sock *sk, struct request_sock *req,
 out:
 	spin_unlock_bh(&fastopenq->lock);
 }
+EXPORT_SYMBOL(reqsk_fastopen_remove);
diff --git a/net/core/skbuff.c b/net/core/skbuff.c
index 564beb7e..ac5ba646 100644
--- a/net/core/skbuff.c
+++ b/net/core/skbuff.c
@@ -78,7 +78,9 @@
 #include <linux/user_namespace.h>
 
 struct kmem_cache *skbuff_head_cache __read_mostly;
+#ifndef CONFIG_SECURITY_TEMPESTA
 static struct kmem_cache *skbuff_fclone_cache __read_mostly;
+#endif
 int sysctl_max_skb_frags __read_mostly = MAX_SKB_FRAGS;
 EXPORT_SYMBOL(sysctl_max_skb_frags);
 
@@ -114,6 +116,7 @@ static void skb_under_panic(struct sk_buff *skb, unsigned int sz, void *addr)
 	skb_panic(skb, sz, addr, __func__);
 }
 
+#ifndef CONFIG_SECURITY_TEMPESTA
 /*
  * kmalloc_reserve is a wrapper around kmalloc_node_track_caller that tells
  * the caller if emergency pfmemalloc reserves are being used. If it is and
@@ -150,6 +153,218 @@ static void *__kmalloc_reserve(size_t size, gfp_t flags, int node,
 
 	return obj;
 }
+#else
+/*
+ * Chunks of size 512B, 1KB and 2KB.
+ * Typical sk_buff requires ~272B or ~552B (for fclone),
+ * skb_shared_info is ~320B.
+ */
+#define PG_LISTS_N		3
+#define PG_CHUNK_BITS		(PAGE_SHIFT - 3)
+#define PG_CHUNK_SZ		(1 << PG_CHUNK_BITS)
+#define PG_CHUNK_MASK		(~(PG_CHUNK_SZ - 1))
+#define PG_ALLOC_SZ(s)		(((s) + (PG_CHUNK_SZ - 1)) & PG_CHUNK_MASK)
+#define PG_CHUNK_NUM(s)		(PG_ALLOC_SZ(s) >> PG_CHUNK_BITS)
+#define PG_POOL_HLIM_BASE	256
+
+/**
+ * @lh		- list head of chunk pool;
+ * @count	- current number of chunks in @lh;
+ * @h_limit	- hard limit for size of @lh;
+ * @max		- current maximum allowed size of the list, can be 0.
+ */
+typedef struct {
+	struct list_head	lh;
+	unsigned int		count;
+	unsigned int		h_limit;
+	unsigned int		max;
+} TfwSkbMemPool;
+
+static DEFINE_PER_CPU(TfwSkbMemPool [PG_LISTS_N], pg_mpool);
+
+static bool
+__pg_pool_grow(TfwSkbMemPool *pool)
+{
+	if (!pool->count) {
+		/* Too few chunks were provisioned. */
+		unsigned int n = max(pool->max, 1U) << 1; /* start from 2 */
+		pool->max = (n > pool->h_limit) ? pool->h_limit : n;
+		return false;
+	}
+	if (pool->max < pool->h_limit)
+		++pool->max;
+	return true;
+}
+
+static bool
+__pg_pool_shrink(TfwSkbMemPool *pool)
+{
+	if (unlikely(pool->count >= pool->max)) {
+		/* Producers are much faster consumers right now. */
+		pool->max >>= 1;
+		while (pool->count > pool->max) {
+			struct list_head *pc = pool->lh.next;
+			list_del(pc);
+			put_page(virt_to_page(pc));
+			--pool->count;
+		}
+		return false;
+	}
+	/*
+	 * Producers and consumers look balanced.
+	 * Slowly reduce provisioning.
+	 */
+	if (pool->max)
+		--pool->max;
+	return true;
+}
+
+static void *
+__pg_skb_alloc(unsigned int size, gfp_t gfp_mask, int node)
+{
+	/*
+	 * Don't disable softirq if hardirqs are already disabled to avoid
+	 * warning in __local_bh_enable_ip(). Disable user space process
+	 * preemption as well as preemption by softirq (see SOFTIRQ_LOCK_OFFSET
+	 * usage in spin locks for the same motivation).
+	 */
+	bool dolock = !(in_irq() || irqs_disabled());
+#define PREEMPT_CTX_DISABLE()						\
+do {									\
+	if (dolock)							\
+		local_bh_disable();					\
+	preempt_disable();						\
+} while (0)
+
+#define PREEMPT_CTX_ENABLE()						\
+do {									\
+	preempt_enable();						\
+	if (dolock)							\
+		local_bh_enable();					\
+} while (0)
+
+	char *ptr;
+	struct page *pg;
+	TfwSkbMemPool *pools;
+	unsigned int c, cn, o, l, po;
+
+	cn = PG_CHUNK_NUM(size);
+	po = get_order(PG_ALLOC_SZ(size));
+
+	PREEMPT_CTX_DISABLE();
+
+	pools = this_cpu_ptr(pg_mpool);
+
+	for (o = (cn == 1) ? 0 : (cn == 2) ? 1 : (cn <= 4) ? 2 : PG_LISTS_N;
+	     o < PG_LISTS_N; ++o)
+	{
+		struct list_head *pc;
+		if (!__pg_pool_grow(&pools[o]))
+			continue;
+
+		pc = pools[o].lh.next;
+		list_del(pc);
+		--pools[o].count;
+		ptr = (char *)pc;
+		pg = virt_to_page(ptr);
+		goto assign_tail_chunks;
+	}
+
+	PREEMPT_CTX_ENABLE();
+
+	/*
+	 * Add compound page metadata, if page order is > 0.
+	 * Don't use __GFP_NOMEMALLOC to allow caller access to reserved pools if
+	 * it requested so.
+	 */
+	gfp_mask |= __GFP_NOWARN | __GFP_NORETRY | (po ? __GFP_COMP : 0);
+	pg = alloc_pages_node(node, gfp_mask, po);
+	if (!pg)
+		return NULL;
+	ptr = (char *)page_address(pg);
+	/*
+	 * Don't try to split compound page. Also don't try to reuse pages
+	 * from reserved memory areas to put and free them quicker.
+	 *
+	 * TODO compound pages can be split as __alloc_page_frag() does it
+	 * using fragment size in page reference counter. Large messages
+	 * (e.g. large HTML pages returned by a backend server) go this way
+	 * and allocate compound pages.
+	 */
+	if (po || page_is_pfmemalloc(pg))
+		return ptr;
+	o = PAGE_SHIFT - PG_CHUNK_BITS;
+
+	PREEMPT_CTX_DISABLE();
+
+	pools = this_cpu_ptr(pg_mpool);
+
+assign_tail_chunks:
+	/* Split and store small tail chunks. */
+	for (c = cn, cn = 1 << o, l = PG_LISTS_N - 1; c < cn; c += (1 << l)) {
+		struct list_head *chunk;
+		while (c + (1 << l) > cn)
+			--l;
+		chunk = (struct list_head *)(ptr + PG_CHUNK_SZ * c);
+		if (__pg_pool_shrink(&pools[l])) {
+			get_page(pg);
+			list_add(chunk, &pools[l].lh);
+			++pools[l].count;
+		}
+	}
+
+	PREEMPT_CTX_ENABLE();
+
+	return ptr;
+#undef PREEMPT_CTX_DISABLE
+#undef PREEMPT_CTX_ENABLE
+}
+#endif
+
+static void
+__alloc_skb_init(struct sk_buff *skb, u8 *data, unsigned int size,
+		 int flags, bool pfmemalloc)
+{
+	struct skb_shared_info *shinfo;
+
+	/*
+	 * Only clear those fields we need to clear, not those that we will
+	 * actually initialise below. Hence, don't put any more fields after
+	 * the tail pointer in struct sk_buff!
+	 */
+	bzero_fast(skb, offsetof(struct sk_buff, tail));
+	/* Account for allocated memory : skb + skb->head */
+	skb->truesize = SKB_TRUESIZE(size);
+	skb->pfmemalloc = pfmemalloc;
+	refcount_set(&skb->users, 1);
+	skb->head = data;
+	skb->data = data;
+	skb_reset_tail_pointer(skb);
+	skb->end = skb->tail + size;
+	skb->mac_header = (typeof(skb->mac_header))~0U;
+	skb->transport_header = (typeof(skb->transport_header))~0U;
+
+	/* make sure we initialize shinfo sequentially */
+	shinfo = skb_shinfo(skb);
+	bzero_fast(shinfo, offsetof(struct skb_shared_info, dataref));
+	atomic_set(&shinfo->dataref, 1);
+
+	if (flags & SKB_ALLOC_FCLONE) {
+		struct sk_buff_fclones *fclones;
+
+		fclones = container_of(skb, struct sk_buff_fclones, skb1);
+
+		skb->fclone = SKB_FCLONE_ORIG;
+		refcount_set(&fclones->fclone_ref, 1);
+
+		fclones->skb2.fclone = SKB_FCLONE_CLONE;
+#ifdef CONFIG_SECURITY_TEMPESTA
+		fclones->skb2.skb_page = 1;
+		fclones->skb2.head_frag = 1;
+#endif
+	}
+}
+
 
 /* 	Allocate a new skbuff. We do this ourselves so we can fill in a few
  *	'private' fields and also do memory statistics to find all the
@@ -174,11 +389,11 @@ static void *__kmalloc_reserve(size_t size, gfp_t flags, int node,
  *	Buffers may only be allocated from interrupts using a @gfp_mask of
  *	%GFP_ATOMIC.
  */
+#ifndef CONFIG_SECURITY_TEMPESTA
 struct sk_buff *__alloc_skb(unsigned int size, gfp_t gfp_mask,
 			    int flags, int node)
 {
 	struct kmem_cache *cache;
-	struct skb_shared_info *shinfo;
 	struct sk_buff *skb;
 	u8 *data;
 	bool pfmemalloc;
@@ -212,38 +427,7 @@ struct sk_buff *__alloc_skb(unsigned int size, gfp_t gfp_mask,
 	size = SKB_WITH_OVERHEAD(ksize(data));
 	prefetchw(data + size);
 
-	/*
-	 * Only clear those fields we need to clear, not those that we will
-	 * actually initialise below. Hence, don't put any more fields after
-	 * the tail pointer in struct sk_buff!
-	 */
-	memset(skb, 0, offsetof(struct sk_buff, tail));
-	/* Account for allocated memory : skb + skb->head */
-	skb->truesize = SKB_TRUESIZE(size);
-	skb->pfmemalloc = pfmemalloc;
-	refcount_set(&skb->users, 1);
-	skb->head = data;
-	skb->data = data;
-	skb_reset_tail_pointer(skb);
-	skb->end = skb->tail + size;
-	skb->mac_header = (typeof(skb->mac_header))~0U;
-	skb->transport_header = (typeof(skb->transport_header))~0U;
-
-	/* make sure we initialize shinfo sequentially */
-	shinfo = skb_shinfo(skb);
-	memset(shinfo, 0, offsetof(struct skb_shared_info, dataref));
-	atomic_set(&shinfo->dataref, 1);
-
-	if (flags & SKB_ALLOC_FCLONE) {
-		struct sk_buff_fclones *fclones;
-
-		fclones = container_of(skb, struct sk_buff_fclones, skb1);
-
-		skb->fclone = SKB_FCLONE_ORIG;
-		refcount_set(&fclones->fclone_ref, 1);
-
-		fclones->skb2.fclone = SKB_FCLONE_CLONE;
-	}
+	__alloc_skb_init(skb, data, size, flags, pfmemalloc);
 out:
 	return skb;
 nodata:
@@ -251,6 +435,42 @@ struct sk_buff *__alloc_skb(unsigned int size, gfp_t gfp_mask,
 	skb = NULL;
 	goto out;
 }
+#else
+/**
+ * Tempesta: allocate skb on the same page with data to improve space locality
+ * and make head data fragmentation easier.
+ */
+struct sk_buff *
+__alloc_skb(unsigned int size, gfp_t gfp_mask, int flags, int node)
+{
+	struct sk_buff *skb;
+	struct page *pg;
+	u8 *data;
+	size_t skb_sz = (flags & SKB_ALLOC_FCLONE)
+			? SKB_DATA_ALIGN(sizeof(struct sk_buff_fclones))
+			: SKB_DATA_ALIGN(sizeof(struct sk_buff));
+	size_t shi_sz = SKB_DATA_ALIGN(sizeof(struct skb_shared_info));
+	size_t n = skb_sz + shi_sz + SKB_DATA_ALIGN(size);
+
+	if (sk_memalloc_socks() && (flags & SKB_ALLOC_RX))
+		gfp_mask |= __GFP_MEMALLOC;
+
+	if (!(skb = __pg_skb_alloc(n, gfp_mask, node)))
+		return NULL;
+
+	data = (u8 *)skb + skb_sz;
+	size = SKB_WITH_OVERHEAD(PG_ALLOC_SZ(n) - skb_sz);
+	prefetchw(data + size);
+
+	pg = virt_to_head_page(data);
+	get_page(pg);
+	__alloc_skb_init(skb, data, size, flags, page_is_pfmemalloc(pg));
+	skb->head_frag = 1;
+	skb->skb_page = 1;
+
+	return skb;
+}
+#endif
 EXPORT_SYMBOL(__alloc_skb);
 
 /**
@@ -284,7 +504,7 @@ struct sk_buff *__build_skb(void *data, unsigned int frag_size)
 
 	size -= SKB_DATA_ALIGN(sizeof(struct skb_shared_info));
 
-	memset(skb, 0, offsetof(struct sk_buff, tail));
+	bzero_fast(skb, offsetof(struct sk_buff, tail));
 	skb->truesize = SKB_TRUESIZE(size);
 	refcount_set(&skb->users, 1);
 	skb->head = data;
@@ -296,7 +516,7 @@ struct sk_buff *__build_skb(void *data, unsigned int frag_size)
 
 	/* make sure we initialize shinfo sequentially */
 	shinfo = skb_shinfo(skb);
-	memset(shinfo, 0, offsetof(struct skb_shared_info, dataref));
+	bzero_fast(shinfo, offsetof(struct skb_shared_info, dataref));
 	atomic_set(&shinfo->dataref, 1);
 
 	return skb;
@@ -579,7 +799,12 @@ static void kfree_skbmem(struct sk_buff *skb)
 
 	switch (skb->fclone) {
 	case SKB_FCLONE_UNAVAILABLE:
-		kmem_cache_free(skbuff_head_cache, skb);
+#ifdef CONFIG_SECURITY_TEMPESTA
+		if (skb->skb_page)
+			put_page(virt_to_page(skb));
+		else
+#endif
+			kmem_cache_free(skbuff_head_cache, skb);
 		return;
 
 	case SKB_FCLONE_ORIG:
@@ -600,7 +825,12 @@ static void kfree_skbmem(struct sk_buff *skb)
 	if (!refcount_dec_and_test(&fclones->fclone_ref))
 		return;
 fastpath:
+#ifdef CONFIG_SECURITY_TEMPESTA
+	BUG_ON(!skb->skb_page);
+	put_page(virt_to_page(skb));
+#else
 	kmem_cache_free(skbuff_fclone_cache, fclones);
+#endif
 }
 
 void skb_release_head_state(struct sk_buff *skb)
@@ -735,6 +965,17 @@ static inline void _kfree_skb_defer(struct sk_buff *skb)
 	/* drop skb->head and call any destructors for packet */
 	skb_release_all(skb);
 
+	/*
+	 * Tempesta uses its own fast page allocator for socket buffers,
+	 * so no need to use napi_alloc_cache for paged skbs.
+	 */
+#ifdef CONFIG_SECURITY_TEMPESTA
+	if (skb->skb_page) {
+		put_page(virt_to_page(skb));
+		return;
+	}
+#endif
+
 	/* record skb to CPU local list */
 	nc->skb_cache[nc->skb_count++] = skb;
 
@@ -794,7 +1035,7 @@ static void __copy_skb_header(struct sk_buff *new, const struct sk_buff *old)
 	new->tstamp		= old->tstamp;
 	/* We do not copy old->sk */
 	new->dev		= old->dev;
-	memcpy(new->cb, old->cb, sizeof(old->cb));
+	memcpy_fast(new->cb, old->cb, sizeof(old->cb));
 	skb_dst_copy(new, old);
 #ifdef CONFIG_XFRM
 	new->sp			= secpath_get(old->sp);
@@ -806,9 +1047,9 @@ static void __copy_skb_header(struct sk_buff *new, const struct sk_buff *old)
 	 */
 	new->queue_mapping = old->queue_mapping;
 
-	memcpy(&new->headers_start, &old->headers_start,
-	       offsetof(struct sk_buff, headers_end) -
-	       offsetof(struct sk_buff, headers_start));
+	memcpy_fast(&new->headers_start, &old->headers_start,
+		    offsetof(struct sk_buff, headers_end) -
+		    offsetof(struct sk_buff, headers_start));
 	CHECK_SKB_FIELD(protocol);
 	CHECK_SKB_FIELD(csum);
 	CHECK_SKB_FIELD(hash);
@@ -1051,7 +1292,7 @@ void sock_zerocopy_callback(struct ubuf_info *uarg, bool success)
 	hi = uarg->id + len - 1;
 
 	serr = SKB_EXT_ERR(skb);
-	memset(serr, 0, sizeof(*serr));
+	bzero_fast(serr, sizeof(*serr));
 	serr->ee.ee_errno = 0;
 	serr->ee.ee_origin = SO_EE_ORIGIN_ZEROCOPY;
 	serr->ee.ee_data = hi;
@@ -1217,8 +1458,8 @@ int skb_copy_ubufs(struct sk_buff *skb, gfp_t gfp_mask)
 					page = (struct page *)page_private(page);
 				}
 				copy = min_t(u32, PAGE_SIZE - d_off, p_len - done);
-				memcpy(page_address(page) + d_off,
-				       vaddr + p_off + done, copy);
+				memcpy_fast(page_address(page) + d_off,
+					    vaddr + p_off + done, copy);
 				done += copy;
 				d_off += copy;
 			}
@@ -1272,6 +1513,10 @@ struct sk_buff *skb_clone(struct sk_buff *skb, gfp_t gfp_mask)
 	    refcount_read(&fclones->fclone_ref) == 1) {
 		n = &fclones->skb2;
 		refcount_set(&fclones->fclone_ref, 2);
+#ifdef CONFIG_SECURITY_TEMPESTA
+		BUG_ON(!skb->skb_page);
+		BUG_ON(!n->skb_page);
+#endif
 	} else {
 		if (skb_pfmemalloc(skb))
 			gfp_mask |= __GFP_MEMALLOC;
@@ -1281,6 +1526,9 @@ struct sk_buff *skb_clone(struct sk_buff *skb, gfp_t gfp_mask)
 			return NULL;
 
 		n->fclone = SKB_FCLONE_UNAVAILABLE;
+#ifdef CONFIG_SECURITY_TEMPESTA
+		n->skb_page = 0;
+#endif
 	}
 
 	return __skb_clone(n, skb);
@@ -1452,24 +1700,31 @@ int pskb_expand_head(struct sk_buff *skb, int nhead, int ntail,
 	if (skb_shared(skb))
 		BUG();
 
-	size = SKB_DATA_ALIGN(size);
+	size = SKB_DATA_ALIGN(size)
+	       + SKB_DATA_ALIGN(sizeof(struct skb_shared_info));
 
 	if (skb_pfmemalloc(skb))
 		gfp_mask |= __GFP_MEMALLOC;
-	data = kmalloc_reserve(size + SKB_DATA_ALIGN(sizeof(struct skb_shared_info)),
-			       gfp_mask, NUMA_NO_NODE, NULL);
+#ifdef CONFIG_SECURITY_TEMPESTA
+	data = __pg_skb_alloc(size, gfp_mask, NUMA_NO_NODE);
+	if (!data)
+		goto nodata;
+	size = SKB_WITH_OVERHEAD(PG_ALLOC_SZ(size));
+#else
+	data = kmalloc_reserve(size, gfp_mask, NUMA_NO_NODE, NULL);
 	if (!data)
 		goto nodata;
 	size = SKB_WITH_OVERHEAD(ksize(data));
+#endif
 
 	/* Copy only real data... and, alas, header. This should be
 	 * optimized for the cases when header is void.
 	 */
-	memcpy(data + nhead, skb->head, skb_tail_pointer(skb) - skb->head);
+	memcpy_fast(data + nhead, skb->head, skb_tail_pointer(skb) - skb->head);
 
-	memcpy((struct skb_shared_info *)(data + size),
-	       skb_shinfo(skb),
-	       offsetof(struct skb_shared_info, frags[skb_shinfo(skb)->nr_frags]));
+	memcpy_fast((struct skb_shared_info *)(data + size), skb_shinfo(skb),
+		    offsetof(struct skb_shared_info,
+			     frags[skb_shinfo(skb)->nr_frags]));
 
 	/*
 	 * if shinfo is shared we must drop the old head gracefully, but if it
@@ -1494,7 +1749,12 @@ int pskb_expand_head(struct sk_buff *skb, int nhead, int ntail,
 	off = (data + nhead) - skb->head;
 
 	skb->head     = data;
+#ifdef CONFIG_SECURITY_TEMPESTA
+	skb->head_frag = 1;
+	skb->tail_lock = 0;
+#else
 	skb->head_frag = 0;
+#endif
 	skb->data    += off;
 #ifdef NET_SKBUFF_DATA_USES_OFFSET
 	skb->end      = size;
@@ -1519,7 +1779,11 @@ int pskb_expand_head(struct sk_buff *skb, int nhead, int ntail,
 	return 0;
 
 nofrags:
+#ifdef CONFIG_SECURITY_TEMPESTA
+	put_page(virt_to_page(data));
+#else
 	kfree(data);
+#endif
 nodata:
 	return -ENOMEM;
 }
@@ -1626,11 +1890,15 @@ int __skb_pad(struct sk_buff *skb, int pad, bool free_on_error)
 
 	/* If the skbuff is non linear tailroom is always zero.. */
 	if (!skb_cloned(skb) && skb_tailroom(skb) >= pad) {
-		memset(skb->data+skb->len, 0, pad);
+		bzero_fast(skb->data+skb->len, pad);
 		return 0;
 	}
 
+#ifdef CONFIG_SECURITY_TEMPESTA
+	ntail = skb->data_len + pad - skb_tailroom_locked(skb);
+#else
 	ntail = skb->data_len + pad - (skb->end - skb->tail);
+#endif
 	if (likely(skb_cloned(skb) || ntail > 0)) {
 		err = pskb_expand_head(skb, 0, ntail, GFP_ATOMIC);
 		if (unlikely(err))
@@ -1644,7 +1912,7 @@ int __skb_pad(struct sk_buff *skb, int pad, bool free_on_error)
 	if (unlikely(err))
 		goto free_skb;
 
-	memset(skb->data + skb->len, 0, pad);
+	bzero_fast(skb->data + skb->len, pad);
 	return 0;
 
 free_skb:
@@ -1868,7 +2136,13 @@ void *__pskb_pull_tail(struct sk_buff *skb, int delta)
 	 * plus 128 bytes for future expansions. If we have enough
 	 * room at tail, reallocate without expansion only if skb is cloned.
 	 */
-	int i, k, eat = (skb->tail + delta) - skb->end;
+	int i, k, eat;
+
+#ifdef CONFIG_SECURITY_TEMPESTA
+	eat = delta - skb_tailroom_locked(skb);
+#else
+	eat = (skb->tail + delta) - skb->end;
+#endif
 
 	if (eat > 0 || skb_cloned(skb)) {
 		if (pskb_expand_head(skb, 0, eat > 0 ? eat + 128 : 0,
@@ -2039,7 +2313,7 @@ int skb_copy_bits(const struct sk_buff *skb, int offset, void *to, int len)
 					      f->page_offset + offset - start,
 					      copy, p, p_off, p_len, copied) {
 				vaddr = kmap_atomic(p);
-				memcpy(to + copied, vaddr + p_off, p_len);
+				memcpy_fast(to + copied, vaddr + p_off, p_len);
 				kunmap_atomic(vaddr);
 			}
 
@@ -2098,8 +2372,8 @@ static struct page *linear_to_page(struct page *page, unsigned int *len,
 
 	*len = min_t(unsigned int, *len, pfrag->size - pfrag->offset);
 
-	memcpy(page_address(pfrag->page) + pfrag->offset,
-	       page_address(page) + *offset, *len);
+	memcpy_fast(page_address(pfrag->page) + pfrag->offset,
+		    page_address(page) + *offset, *len);
 	*offset = pfrag->offset;
 	pfrag->offset += *len;
 
@@ -2280,7 +2554,7 @@ int skb_send_sock_locked(struct sock *sk, struct sk_buff *skb, int offset,
 		slen = min_t(int, len, skb_headlen(skb) - offset);
 		kv.iov_base = skb->data + offset;
 		kv.iov_len = slen;
-		memset(&msg, 0, sizeof(msg));
+		bzero_fast(&msg, sizeof(msg));
 
 		ret = kernel_sendmsg_locked(sk, &msg, &kv, 1, slen);
 		if (ret <= 0)
@@ -2412,7 +2686,7 @@ int skb_store_bits(struct sk_buff *skb, int offset, const void *from, int len)
 					      frag->page_offset + offset - start,
 					      copy, p, p_off, p_len, copied) {
 				vaddr = kmap_atomic(p);
-				memcpy(vaddr + p_off, from + copied, p_len);
+				memcpy_fast(vaddr + p_off, from + copied, p_len);
 				kunmap_atomic(vaddr);
 			}
 
@@ -3841,7 +4115,8 @@ int skb_gro_receive(struct sk_buff **head, struct sk_buff *skb)
 		frag->page_offset = first_offset;
 		skb_frag_size_set(frag, first_size);
 
-		memcpy(frag + 1, skbinfo->frags, sizeof(*frag) * skbinfo->nr_frags);
+		memcpy_fast(frag + 1, skbinfo->frags,
+			    sizeof(*frag) * skbinfo->nr_frags);
 		/* We dont need to clear skbinfo->nr_frags here */
 
 		delta_truesize = skb->truesize - SKB_DATA_ALIGN(sizeof(struct sk_buff));
@@ -3888,16 +4163,31 @@ EXPORT_SYMBOL_GPL(skb_gro_receive);
 
 void __init skb_init(void)
 {
-	skbuff_head_cache = kmem_cache_create("skbuff_head_cache",
-					      sizeof(struct sk_buff),
-					      0,
-					      SLAB_HWCACHE_ALIGN|SLAB_PANIC,
-					      NULL);
+#ifdef CONFIG_SECURITY_TEMPESTA
+	int cpu, l;
+	for_each_possible_cpu(cpu)
+		for (l = 0; l < PG_LISTS_N; ++l) {
+			TfwSkbMemPool *pool = per_cpu_ptr(&pg_mpool[l], cpu);
+			INIT_LIST_HEAD(&pool->lh);
+			/*
+			 * Large chunks are also can be used to get smaller
+			 * chunks, so we cache them more aggressively.
+			 */
+			pool->h_limit = PG_POOL_HLIM_BASE << l;
+		}
+#else
 	skbuff_fclone_cache = kmem_cache_create("skbuff_fclone_cache",
 						sizeof(struct sk_buff_fclones),
 						0,
 						SLAB_HWCACHE_ALIGN|SLAB_PANIC,
 						NULL);
+#endif
+
+	skbuff_head_cache = kmem_cache_create("skbuff_head_cache",
+					      sizeof(struct sk_buff),
+					      0,
+					      SLAB_HWCACHE_ALIGN|SLAB_PANIC,
+					      NULL);
 }
 
 static int
@@ -4253,7 +4543,7 @@ static void __skb_complete_tx_timestamp(struct sk_buff *skb,
 	BUILD_BUG_ON(sizeof(struct sock_exterr_skb) > sizeof(skb->cb));
 
 	serr = SKB_EXT_ERR(skb);
-	memset(serr, 0, sizeof(*serr));
+	bzero_fast(serr, sizeof(*serr));
 	serr->ee.ee_errno = ENOMSG;
 	serr->ee.ee_origin = SO_EE_ORIGIN_TIMESTAMPING;
 	serr->ee.ee_info = tstype;
@@ -4376,7 +4666,7 @@ void skb_complete_wifi_ack(struct sk_buff *skb, bool acked)
 	skb->wifi_acked = acked;
 
 	serr = SKB_EXT_ERR(skb);
-	memset(serr, 0, sizeof(*serr));
+	bzero_fast(serr, sizeof(*serr));
 	serr->ee.ee_errno = ENOMSG;
 	serr->ee.ee_origin = SO_EE_ORIGIN_TXSTATUS;
 
@@ -4749,7 +5039,15 @@ void kfree_skb_partial(struct sk_buff *skb, bool head_stolen)
 {
 	if (head_stolen) {
 		skb_release_head_state(skb);
+#ifdef CONFIG_SECURITY_TEMPESTA
+		/*
+		 * fclones are possible here with Tempesta due to using
+		 * pskb_copy_for_clone() in ss_send().
+		 */
+		kfree_skbmem(skb);
+#else
 		kmem_cache_free(skbuff_head_cache, skb);
+#endif
 	} else {
 		__kfree_skb(skb);
 	}
@@ -4814,9 +5112,9 @@ bool skb_try_coalesce(struct sk_buff *to, struct sk_buff *from,
 
 	WARN_ON_ONCE(delta < len);
 
-	memcpy(skb_shinfo(to)->frags + skb_shinfo(to)->nr_frags,
-	       skb_shinfo(from)->frags,
-	       skb_shinfo(from)->nr_frags * sizeof(skb_frag_t));
+	memcpy_fast(skb_shinfo(to)->frags + skb_shinfo(to)->nr_frags,
+		    skb_shinfo(from)->frags,
+		    skb_shinfo(from)->nr_frags * sizeof(skb_frag_t));
 	skb_shinfo(to)->nr_frags += skb_shinfo(from)->nr_frags;
 
 	if (!skb_cloned(from))
@@ -5194,26 +5492,36 @@ static int pskb_carve_inside_header(struct sk_buff *skb, const u32 off,
 
 	if (skb_pfmemalloc(skb))
 		gfp_mask |= __GFP_MEMALLOC;
+#ifdef CONFIG_SECURITY_TEMPESTA
+	size += SKB_DATA_ALIGN(sizeof(struct skb_shared_info));
+	data = __pg_skb_alloc(size, gfp_mask, NUMA_NO_NODE);
+	if (!data)
+		return -ENOMEM;
+	size = SKB_WITH_OVERHEAD(PG_ALLOC_SZ(size));
+#else
 	data = kmalloc_reserve(size +
 			       SKB_DATA_ALIGN(sizeof(struct skb_shared_info)),
 			       gfp_mask, NUMA_NO_NODE, NULL);
 	if (!data)
 		return -ENOMEM;
-
 	size = SKB_WITH_OVERHEAD(ksize(data));
+#endif
 
 	/* Copy real data, and all frags */
 	skb_copy_from_linear_data_offset(skb, off, data, new_hlen);
 	skb->len -= off;
 
-	memcpy((struct skb_shared_info *)(data + size),
-	       skb_shinfo(skb),
-	       offsetof(struct skb_shared_info,
-			frags[skb_shinfo(skb)->nr_frags]));
+	memcpy_fast((struct skb_shared_info *)(data + size), skb_shinfo(skb),
+		    offsetof(struct skb_shared_info,
+			     frags[skb_shinfo(skb)->nr_frags]));
 	if (skb_cloned(skb)) {
 		/* drop the old head gracefully */
 		if (skb_orphan_frags(skb, gfp_mask)) {
+#ifdef CONFIG_SECURITY_TEMPESTA
+			skb_free_frag(data);
+#else
 			kfree(data);
+#endif
 			return -ENOMEM;
 		}
 		for (i = 0; i < skb_shinfo(skb)->nr_frags; i++)
@@ -5230,7 +5538,11 @@ static int pskb_carve_inside_header(struct sk_buff *skb, const u32 off,
 
 	skb->head = data;
 	skb->data = data;
+#ifdef CONFIG_SECURITY_TEMPESTA
+	skb->head_frag = 1;
+#else
 	skb->head_frag = 0;
+#endif
 #ifdef NET_SKBUFF_DATA_USES_OFFSET
 	skb->end = size;
 #else
@@ -5318,19 +5630,30 @@ static int pskb_carve_inside_nonlinear(struct sk_buff *skb, const u32 off,
 
 	if (skb_pfmemalloc(skb))
 		gfp_mask |= __GFP_MEMALLOC;
+#ifdef CONFIG_SECURITY_TEMPESTA
+	size += SKB_DATA_ALIGN(sizeof(struct skb_shared_info));
+	data = __pg_skb_alloc(size, gfp_mask, NUMA_NO_NODE);
+	if (!data)
+		return -ENOMEM;
+	size = SKB_WITH_OVERHEAD(PG_ALLOC_SZ(size));
+#else
 	data = kmalloc_reserve(size +
 			       SKB_DATA_ALIGN(sizeof(struct skb_shared_info)),
 			       gfp_mask, NUMA_NO_NODE, NULL);
 	if (!data)
 		return -ENOMEM;
-
 	size = SKB_WITH_OVERHEAD(ksize(data));
+#endif
 
-	memcpy((struct skb_shared_info *)(data + size),
-	       skb_shinfo(skb), offsetof(struct skb_shared_info,
-					 frags[skb_shinfo(skb)->nr_frags]));
+	memcpy_fast((struct skb_shared_info *)(data + size),
+		    skb_shinfo(skb), offsetof(struct skb_shared_info,
+					      frags[skb_shinfo(skb)->nr_frags]));
 	if (skb_orphan_frags(skb, gfp_mask)) {
+#ifdef CONFIG_SECURITY_TEMPESTA
+		skb_free_frag(data);
+#else
 		kfree(data);
+#endif
 		return -ENOMEM;
 	}
 	shinfo = (struct skb_shared_info *)(data + size);
@@ -5368,8 +5691,12 @@ static int pskb_carve_inside_nonlinear(struct sk_buff *skb, const u32 off,
 	skb_release_data(skb);
 
 	skb->head = data;
-	skb->head_frag = 0;
 	skb->data = data;
+#ifdef CONFIG_SECURITY_TEMPESTA
+	skb->head_frag = 1;
+#else
+	skb->head_frag = 0;
+#endif
 #ifdef NET_SKBUFF_DATA_USES_OFFSET
 	skb->end = size;
 #else
diff --git a/net/core/sock.c b/net/core/sock.c
index ec6eb546..4ffa79ba 100644
--- a/net/core/sock.c
+++ b/net/core/sock.c
@@ -1441,10 +1441,11 @@ static void sock_copy(struct sock *nsk, const struct sock *osk)
 #ifdef CONFIG_SECURITY_NETWORK
 	void *sptr = nsk->sk_security;
 #endif
-	memcpy(nsk, osk, offsetof(struct sock, sk_dontcopy_begin));
+	memcpy_fast(nsk, osk, offsetof(struct sock, sk_dontcopy_begin));
 
-	memcpy(&nsk->sk_dontcopy_end, &osk->sk_dontcopy_end,
-	       osk->sk_prot->obj_size - offsetof(struct sock, sk_dontcopy_end));
+	memcpy_fast(&nsk->sk_dontcopy_end, &osk->sk_dontcopy_end,
+		    osk->sk_prot->obj_size - offsetof(struct sock,
+						      sk_dontcopy_end));
 
 #ifdef CONFIG_SECURITY_NETWORK
 	nsk->sk_security = sptr;
@@ -1806,8 +1807,16 @@ void sock_wfree(struct sk_buff *skb)
 	 * if sk_wmem_alloc reaches 0, we must finish what sk_free()
 	 * could not do because of in-flight packets
 	 */
-	if (refcount_sub_and_test(len, &sk->sk_wmem_alloc))
+	if (refcount_sub_and_test(len, &sk->sk_wmem_alloc)) {
+		/*
+		 * We don't bother with Tempesta socket memory limitations
+		 * since in proxy mode we just forward packets instead of real
+		 * allocations. Probably this is an issue. Probably sockets
+		 * can be freed from under us.
+		 */
+		WARN_ON(sock_flag(sk, SOCK_TEMPESTA));
 		__sk_free(sk);
+	}
 }
 EXPORT_SYMBOL(sock_wfree);
 
diff --git a/net/ipv4/inet_connection_sock.c b/net/ipv4/inet_connection_sock.c
index 0cc08c51..631fe59d 100644
--- a/net/ipv4/inet_connection_sock.c
+++ b/net/ipv4/inet_connection_sock.c
@@ -806,7 +806,8 @@ struct sock *inet_csk_clone_lock(const struct sock *sk,
 		newicsk->icsk_probes_out  = 0;
 
 		/* Deinitialize accept_queue to trap illegal accesses. */
-		memset(&newicsk->icsk_accept_queue, 0, sizeof(newicsk->icsk_accept_queue));
+		bzero_fast(&newicsk->icsk_accept_queue,
+			   sizeof(newicsk->icsk_accept_queue));
 
 		security_inet_csk_clone(newsk, req);
 	}
@@ -925,6 +926,14 @@ struct sock *inet_csk_reqsk_queue_add(struct sock *sk,
 {
 	struct request_sock_queue *queue = &inet_csk(sk)->icsk_accept_queue;
 
+#ifdef CONFIG_SECURITY_TEMPESTA
+	if (sk->sk_state == TCP_LISTEN && sock_flag(sk, SOCK_TEMPESTA)) {
+		/* Tempesta doesn't use accept queue, just put the request. */
+		reqsk_put(req);
+		return child;
+	}
+#endif
+
 	spin_lock(&queue->rskq_lock);
 	if (unlikely(sk->sk_state != TCP_LISTEN)) {
 		inet_child_forget(sk, req, child);
diff --git a/net/ipv4/inet_hashtables.c b/net/ipv4/inet_hashtables.c
index e7d15fb0..09a21845 100644
--- a/net/ipv4/inet_hashtables.c
+++ b/net/ipv4/inet_hashtables.c
@@ -616,7 +616,8 @@ int __inet_hash_connect(struct inet_timewait_death_row *death_row,
 		goto ok;
 next_port:
 		spin_unlock_bh(&head->lock);
-		cond_resched();
+		if (!in_serving_softirq())
+			cond_resched();
 	}
 
 	offset++;
diff --git a/net/ipv4/ip_fragment.c b/net/ipv4/ip_fragment.c
index df8fe050..2823475a 100644
--- a/net/ipv4/ip_fragment.c
+++ b/net/ipv4/ip_fragment.c
@@ -719,7 +719,7 @@ struct sk_buff *ip_check_defrag(struct net *net, struct sk_buff *skb, u32 user)
 				return skb;
 			if (pskb_trim_rcsum(skb, netoff + len))
 				return skb;
-			memset(IPCB(skb), 0, sizeof(struct inet_skb_parm));
+			bzero_fast(IPCB(skb), sizeof(struct inet_skb_parm));
 			if (ip_defrag(net, skb, user))
 				return NULL;
 			skb_clear_hash(skb);
diff --git a/net/ipv4/ip_input.c b/net/ipv4/ip_input.c
index 57fc13c6..1e519e9a 100644
--- a/net/ipv4/ip_input.c
+++ b/net/ipv4/ip_input.c
@@ -484,7 +484,7 @@ int ip_rcv(struct sk_buff *skb, struct net_device *dev, struct packet_type *pt,
 	skb->transport_header = skb->network_header + iph->ihl*4;
 
 	/* Remove any debris in the socket control block */
-	memset(IPCB(skb), 0, sizeof(struct inet_skb_parm));
+	bzero_fast(IPCB(skb), sizeof(struct inet_skb_parm));
 	IPCB(skb)->iif = skb->skb_iif;
 
 	/* Must drop socket now because of tproxy. */
diff --git a/net/ipv4/ip_output.c b/net/ipv4/ip_output.c
index e8e675be..42d4b07b 100644
--- a/net/ipv4/ip_output.c
+++ b/net/ipv4/ip_output.c
@@ -418,8 +418,8 @@ static void ip_copy_addrs(struct iphdr *iph, const struct flowi4 *fl4)
 {
 	BUILD_BUG_ON(offsetof(typeof(*fl4), daddr) !=
 		     offsetof(typeof(*fl4), saddr) + sizeof(fl4->saddr));
-	memcpy(&iph->saddr, &fl4->saddr,
-	       sizeof(fl4->saddr) + sizeof(fl4->daddr));
+	memcpy_fast(&iph->saddr, &fl4->saddr,
+		    sizeof(fl4->saddr) + sizeof(fl4->daddr));
 }
 
 /* Note: skb->sk can be different from sk, in case of tunnels */
@@ -658,7 +658,8 @@ int ip_do_fragment(struct net *net, struct sock *sk, struct sk_buff *skb,
 				skb_reset_transport_header(frag);
 				__skb_push(frag, hlen);
 				skb_reset_network_header(frag);
-				memcpy(skb_network_header(frag), iph, hlen);
+				memcpy_fast(skb_network_header(frag), iph,
+					    hlen);
 				iph = ip_hdr(frag);
 				iph->tot_len = htons(frag->len);
 				ip_copy_metadata(frag, skb);
@@ -1112,7 +1113,8 @@ static int ip_setup_cork(struct sock *sk, struct inet_cork *cork,
 			if (unlikely(!cork->opt))
 				return -ENOBUFS;
 		}
-		memcpy(cork->opt, &opt->opt, sizeof(struct ip_options) + opt->opt.optlen);
+		memcpy_fast(cork->opt, &opt->opt,
+			    sizeof(struct ip_options) + opt->opt.optlen);
 		cork->flags |= IPCORK_OPT;
 		cork->addr = ipc->addr;
 	}
diff --git a/net/ipv4/tcp.c b/net/ipv4/tcp.c
index 38b9a627..c24be3de 100644
--- a/net/ipv4/tcp.c
+++ b/net/ipv4/tcp.c
@@ -625,18 +625,19 @@ int tcp_ioctl(struct sock *sk, int cmd, unsigned long arg)
 }
 EXPORT_SYMBOL(tcp_ioctl);
 
-static inline void tcp_mark_push(struct tcp_sock *tp, struct sk_buff *skb)
+void tcp_mark_push(struct tcp_sock *tp, struct sk_buff *skb)
 {
 	TCP_SKB_CB(skb)->tcp_flags |= TCPHDR_PSH;
 	tp->pushed_seq = tp->write_seq;
 }
+EXPORT_SYMBOL(tcp_mark_push);
 
 static inline bool forced_push(const struct tcp_sock *tp)
 {
 	return after(tp->write_seq, tp->pushed_seq + (tp->max_window >> 1));
 }
 
-static void skb_entail(struct sock *sk, struct sk_buff *skb)
+void skb_entail(struct sock *sk, struct sk_buff *skb)
 {
 	struct tcp_sock *tp = tcp_sk(sk);
 	struct tcp_skb_cb *tcb = TCP_SKB_CB(skb);
@@ -645,7 +646,11 @@ static void skb_entail(struct sock *sk, struct sk_buff *skb)
 	tcb->seq     = tcb->end_seq = tp->write_seq;
 	tcb->tcp_flags = TCPHDR_ACK;
 	tcb->sacked  = 0;
-	__skb_header_release(skb);
+	/*
+	 * fclones are possible here, so accurately update
+	 * skb_shinfo(skb)->dataref.
+	 */
+	skb_header_release(skb);
 	tcp_add_write_queue_tail(sk, skb);
 	sk->sk_wmem_queued += skb->truesize;
 	sk_mem_charge(sk, skb->truesize);
@@ -654,6 +659,7 @@ static void skb_entail(struct sock *sk, struct sk_buff *skb)
 
 	tcp_slow_start_after_idle_check(sk);
 }
+EXPORT_SYMBOL(skb_entail);
 
 static inline void tcp_mark_urg(struct tcp_sock *tp, int flags)
 {
@@ -680,8 +686,8 @@ static bool tcp_should_autocork(struct sock *sk, struct sk_buff *skb,
 	       refcount_read(&sk->sk_wmem_alloc) > skb->truesize;
 }
 
-static void tcp_push(struct sock *sk, int flags, int mss_now,
-		     int nonagle, int size_goal)
+void tcp_push(struct sock *sk, int flags, int mss_now, int nonagle,
+	      int size_goal)
 {
 	struct tcp_sock *tp = tcp_sk(sk);
 	struct sk_buff *skb;
@@ -714,6 +720,7 @@ static void tcp_push(struct sock *sk, int flags, int mss_now,
 
 	__tcp_push_pending_frames(sk, mss_now, nonagle);
 }
+EXPORT_SYMBOL(tcp_push);
 
 static int tcp_splice_data_recv(read_descriptor_t *rd_desc, struct sk_buff *skb,
 				unsigned int offset, size_t len)
@@ -904,7 +911,7 @@ static unsigned int tcp_xmit_size_goal(struct sock *sk, u32 mss_now,
 	return max(size_goal, mss_now);
 }
 
-static int tcp_send_mss(struct sock *sk, int *size_goal, int flags)
+int tcp_send_mss(struct sock *sk, int *size_goal, int flags)
 {
 	int mss_now;
 
@@ -913,6 +920,7 @@ static int tcp_send_mss(struct sock *sk, int *size_goal, int flags)
 
 	return mss_now;
 }
+EXPORT_SYMBOL(tcp_send_mss);
 
 ssize_t do_tcp_sendpages(struct sock *sk, struct page *page, int offset,
 			 size_t size, int flags)
@@ -1522,7 +1530,7 @@ static int tcp_peek_sndq(struct sock *sk, struct msghdr *msg, int len)
  * calculation of whether or not we must ACK for the sake of
  * a window update.
  */
-static void tcp_cleanup_rbuf(struct sock *sk, int copied)
+void tcp_cleanup_rbuf(struct sock *sk, int copied)
 {
 	struct tcp_sock *tp = tcp_sk(sk);
 	bool time_to_ack = false;
@@ -1579,6 +1587,7 @@ static void tcp_cleanup_rbuf(struct sock *sk, int copied)
 	if (time_to_ack)
 		tcp_send_ack(sk);
 }
+EXPORT_SYMBOL(tcp_cleanup_rbuf);
 
 static struct sk_buff *tcp_recv_skb(struct sock *sk, u32 seq, u32 *off)
 {
@@ -2072,7 +2081,7 @@ static const unsigned char new_state[16] = {
   [TCP_NEW_SYN_RECV]	= TCP_CLOSE,	/* should not happen ! */
 };
 
-static int tcp_close_state(struct sock *sk)
+int tcp_close_state(struct sock *sk)
 {
 	int next = (int)new_state[sk->sk_state];
 	int ns = next & TCP_STATE_MASK;
@@ -2081,6 +2090,7 @@ static int tcp_close_state(struct sock *sk)
 
 	return next & TCP_ACTION_FIN;
 }
+EXPORT_SYMBOL(tcp_close_state);
 
 /*
  *	Shutdown the sending side of a connection. Much like close except
@@ -2120,6 +2130,7 @@ bool tcp_check_oom(struct sock *sk, int shift)
 		net_info_ratelimited("out of memory -- consider tuning tcp_mem\n");
 	return too_many_orphans || out_of_socket_memory;
 }
+EXPORT_SYMBOL(tcp_check_oom);
 
 void tcp_close(struct sock *sk, long timeout)
 {
@@ -2367,7 +2378,7 @@ int tcp_disconnect(struct sock *sk, int flags)
 	 */
 	icsk->icsk_ack.rcv_mss = TCP_MIN_MSS;
 	tcp_init_send_head(sk);
-	memset(&tp->rx_opt, 0, sizeof(tp->rx_opt));
+	bzero_fast(&tp->rx_opt, sizeof(tp->rx_opt));
 	__sk_dst_reset(sk);
 	dst_release(sk->sk_rx_dst);
 	sk->sk_rx_dst = NULL;
diff --git a/net/ipv4/tcp_fastopen.c b/net/ipv4/tcp_fastopen.c
index fbbeda64..df67678f 100644
--- a/net/ipv4/tcp_fastopen.c
+++ b/net/ipv4/tcp_fastopen.c
@@ -60,7 +60,7 @@ error:		kfree(ctx);
 		crypto_free_cipher(ctx->tfm);
 		goto error;
 	}
-	memcpy(ctx->key, key, len);
+	memcpy_fast(ctx->key, key, len);
 
 	spin_lock(&tcp_fastopen_ctx_lock);
 
@@ -301,7 +301,7 @@ struct sock *tcp_try_fastopen(struct sock *sk, struct sk_buff *skb,
 	    tcp_fastopen_cookie_gen(req, skb, &valid_foc) &&
 	    foc->len == TCP_FASTOPEN_COOKIE_SIZE &&
 	    foc->len == valid_foc.len &&
-	    !memcmp(foc->val, valid_foc.val, foc->len)) {
+	    !memcmp_fast(foc->val, valid_foc.val, foc->len)) {
 		/* Cookie is valid. Create a (full) child socket to accept
 		 * the data in SYN before returning a SYN-ACK to ack the
 		 * data. If we fail to create the socket, fall back and
diff --git a/net/ipv4/tcp_input.c b/net/ipv4/tcp_input.c
index 14474ace..1d863d02 100644
--- a/net/ipv4/tcp_input.c
+++ b/net/ipv4/tcp_input.c
@@ -654,6 +654,7 @@ void tcp_rcv_space_adjust(struct sock *sk)
 	tp->rcvq_space.seq = tp->copied_seq;
 	tp->rcvq_space.time = tp->tcp_mstamp;
 }
+EXPORT_SYMBOL(tcp_rcv_space_adjust);
 
 /* There is something which you must keep in mind when you analyze the
  * behavior of the tp->ato delayed ack timeout interval.  When a
@@ -4826,7 +4827,7 @@ tcp_collapse(struct sock *sk, struct sk_buff_head *list, struct rb_root *root,
 		if (!nskb)
 			break;
 
-		memcpy(nskb->cb, skb->cb, sizeof(skb->cb));
+		memcpy_fast(nskb->cb, skb->cb, sizeof(skb->cb));
 		TCP_SKB_CB(nskb)->seq = TCP_SKB_CB(nskb)->end_seq = start;
 		if (list)
 			__skb_queue_before(list, skb, nskb);
@@ -6251,7 +6252,7 @@ static void tcp_reqsk_record_syn(const struct sock *sk,
 		copy = kmalloc(len + sizeof(u32), GFP_ATOMIC);
 		if (copy) {
 			copy[0] = len;
-			memcpy(&copy[1], skb_network_header(skb), len);
+			memcpy_fast(&copy[1], skb_network_header(skb), len);
 			req->saved_syn = copy;
 		}
 	}
diff --git a/net/ipv4/tcp_ipv4.c b/net/ipv4/tcp_ipv4.c
index cab4b935..7683d860 100644
--- a/net/ipv4/tcp_ipv4.c
+++ b/net/ipv4/tcp_ipv4.c
@@ -62,6 +62,7 @@
 #include <linux/init.h>
 #include <linux/times.h>
 #include <linux/slab.h>
+#include <linux/tempesta.h>
 
 #include <net/net_namespace.h>
 #include <net/icmp.h>
@@ -159,8 +160,7 @@ int tcp_v4_connect(struct sock *sk, struct sockaddr *uaddr, int addr_len)
 		return -EAFNOSUPPORT;
 
 	nexthop = daddr = usin->sin_addr.s_addr;
-	inet_opt = rcu_dereference_protected(inet->inet_opt,
-					     lockdep_sock_is_held(sk));
+	inet_opt = rcu_dereference_raw(inet->inet_opt);
 	if (inet_opt && inet_opt->opt.srr) {
 		if (!daddr)
 			return -EINVAL;
@@ -623,7 +623,7 @@ static void tcp_v4_send_reset(const struct sock *sk, struct sk_buff *skb)
 		return;
 
 	/* Swap the send and the receive. */
-	memset(&rep, 0, sizeof(rep));
+	bzero_fast(&rep, sizeof(rep));
 	rep.th.dest   = th->source;
 	rep.th.source = th->dest;
 	rep.th.doff   = sizeof(struct tcphdr) / 4;
@@ -637,7 +637,7 @@ static void tcp_v4_send_reset(const struct sock *sk, struct sk_buff *skb)
 				       skb->len - (th->doff << 2));
 	}
 
-	memset(&arg, 0, sizeof(arg));
+	bzero_fast(&arg, sizeof(arg));
 	arg.iov[0].iov_base = (unsigned char *)&rep;
 	arg.iov[0].iov_len  = sizeof(rep.th);
 
@@ -672,7 +672,7 @@ static void tcp_v4_send_reset(const struct sock *sk, struct sk_buff *skb)
 
 
 		genhash = tcp_v4_md5_hash_skb(newhash, key, NULL, skb);
-		if (genhash || memcmp(hash_location, newhash, 16) != 0)
+		if (genhash || memcmp_fast(hash_location, newhash, 16) != 0)
 			goto out;
 
 	}
@@ -747,8 +747,8 @@ static void tcp_v4_send_ack(const struct sock *sk,
 	struct net *net = sock_net(sk);
 	struct ip_reply_arg arg;
 
-	memset(&rep.th, 0, sizeof(struct tcphdr));
-	memset(&arg, 0, sizeof(arg));
+	bzero_fast(&rep.th, sizeof(struct tcphdr));
+	bzero_fast(&arg, sizeof(arg));
 
 	arg.iov[0].iov_base = (unsigned char *)&rep;
 	arg.iov[0].iov_len  = sizeof(rep.th);
@@ -952,8 +952,7 @@ static struct tcp_md5sig_key *tcp_md5_do_lookup_exact(const struct sock *sk,
 	const struct tcp_md5sig_info *md5sig;
 
 	/* caller either holds rcu_read_lock() or socket lock */
-	md5sig = rcu_dereference_check(tp->md5sig_info,
-				       lockdep_sock_is_held(sk));
+	md5sig = rcu_dereference_raw(tp->md5sig_info);
 	if (!md5sig)
 		return NULL;
 #if IS_ENABLED(CONFIG_IPV6)
@@ -963,7 +962,7 @@ static struct tcp_md5sig_key *tcp_md5_do_lookup_exact(const struct sock *sk,
 	hlist_for_each_entry_rcu(key, &md5sig->head, node) {
 		if (key->family != family)
 			continue;
-		if (!memcmp(&key->addr, addr, size) &&
+		if (!memcmp_fast(&key->addr, addr, size) &&
 		    key->prefixlen == prefixlen)
 			return key;
 	}
@@ -993,7 +992,7 @@ int tcp_md5_do_add(struct sock *sk, const union tcp_md5_addr *addr,
 	key = tcp_md5_do_lookup_exact(sk, addr, family, prefixlen);
 	if (key) {
 		/* Pre-existing entry - just update that one. */
-		memcpy(key->key, newkey, newkeylen);
+		memcpy_fast(key->key, newkey, newkeylen);
 		key->keylen = newkeylen;
 		return 0;
 	}
@@ -1018,13 +1017,13 @@ int tcp_md5_do_add(struct sock *sk, const union tcp_md5_addr *addr,
 		return -ENOMEM;
 	}
 
-	memcpy(key->key, newkey, newkeylen);
+	memcpy_fast(key->key, newkey, newkeylen);
 	key->keylen = newkeylen;
 	key->family = family;
 	key->prefixlen = prefixlen;
-	memcpy(&key->addr, addr,
-	       (family == AF_INET6) ? sizeof(struct in6_addr) :
-				      sizeof(struct in_addr));
+	memcpy_fast(&key->addr, addr,
+		    (family == AF_INET6) ? sizeof(struct in6_addr) :
+					   sizeof(struct in_addr));
 	hlist_add_head_rcu(&key->node, &md5sig->head);
 	return 0;
 }
@@ -1112,7 +1111,7 @@ static int tcp_v4_md5_hash_headers(struct tcp_md5sig_pool *hp,
 	bp->len = cpu_to_be16(nbytes);
 
 	_th = (struct tcphdr *)(bp + 1);
-	memcpy(_th, th, sizeof(*th));
+	memcpy_fast(_th, th, sizeof(*th));
 	_th->check = 0;
 
 	sg_init_one(&sg, bp, sizeof(*bp) + sizeof(*th));
@@ -1148,7 +1147,7 @@ static int tcp_v4_md5_hash_hdr(char *md5_hash, const struct tcp_md5sig_key *key,
 clear_hash:
 	tcp_put_md5sig_pool();
 clear_hash_noput:
-	memset(md5_hash, 0, 16);
+	bzero_fast(md5_hash, 16);
 	return 1;
 }
 
@@ -1194,7 +1193,7 @@ int tcp_v4_md5_hash_skb(char *md5_hash, const struct tcp_md5sig_key *key,
 clear_hash:
 	tcp_put_md5sig_pool();
 clear_hash_noput:
-	memset(md5_hash, 0, 16);
+	bzero_fast(md5_hash, 16);
 	return 1;
 }
 EXPORT_SYMBOL(tcp_v4_md5_hash_skb);
@@ -1246,7 +1245,7 @@ static bool tcp_v4_inbound_md5_hash(const struct sock *sk,
 				      hash_expected,
 				      NULL, skb);
 
-	if (genhash || memcmp(hash_location, newhash, 16) != 0) {
+	if (genhash || memcmp_fast(hash_location, newhash, 16) != 0) {
 		NET_INC_STATS(sock_net(sk), LINUX_MIB_TCPMD5FAILURE);
 		net_info_ratelimited("MD5 Hash failed for (%pI4, %d)->(%pI4, %d)%s\n",
 				     &iph->saddr, ntohs(th->source),
@@ -1400,6 +1399,14 @@ struct sock *tcp_v4_syn_recv_sock(const struct sock *sk, struct sk_buff *skb,
 	}
 #endif
 
+#ifdef CONFIG_SECURITY_TEMPESTA
+	/*
+	 * We need already initialized socket addresses,
+	 * so there is no appropriate security hook.
+	 */
+	if (tempesta_new_clntsk(newsk))
+		goto put_and_exit;
+#endif
 	if (__inet_inherit_port(sk, newsk) < 0)
 		goto put_and_exit;
 	*own_req = inet_ehash_nolisten(newsk, req_to_sk(req_unhash));
diff --git a/net/ipv4/tcp_minisocks.c b/net/ipv4/tcp_minisocks.c
index 420fecbb..67e0513a 100644
--- a/net/ipv4/tcp_minisocks.c
+++ b/net/ipv4/tcp_minisocks.c
@@ -333,6 +333,7 @@ void tcp_time_wait(struct sock *sk, int state, int timeo)
 	tcp_update_metrics(sk);
 	tcp_done(sk);
 }
+EXPORT_SYMBOL(tcp_time_wait);
 
 void tcp_twsk_destructor(struct sock *sk)
 {
diff --git a/net/ipv4/tcp_output.c b/net/ipv4/tcp_output.c
index 83d11cd2..f01e83d5 100644
--- a/net/ipv4/tcp_output.c
+++ b/net/ipv4/tcp_output.c
@@ -392,7 +392,7 @@ static void tcp_ecn_send(struct sock *sk, struct sk_buff *skb,
 /* Constructs common control bits of non-data skb. If SYN/FIN is present,
  * auto increment end seqno.
  */
-static void tcp_init_nondata_skb(struct sk_buff *skb, u32 seq, u8 flags)
+void tcp_init_nondata_skb(struct sk_buff *skb, u32 seq, u8 flags)
 {
 	skb->ip_summed = CHECKSUM_PARTIAL;
 	skb->csum = 0;
@@ -407,6 +407,7 @@ static void tcp_init_nondata_skb(struct sk_buff *skb, u32 seq, u8 flags)
 		seq++;
 	TCP_SKB_CB(skb)->end_seq = seq;
 }
+EXPORT_SYMBOL(tcp_init_nondata_skb);
 
 static inline bool tcp_urg_mode(const struct tcp_sock *tp)
 {
@@ -529,7 +530,7 @@ static void tcp_options_write(__be32 *ptr, struct tcp_sock *tp,
 			*p++ = len;
 		}
 
-		memcpy(p, foc->val, foc->len);
+		memcpy_fast(p, foc->val, foc->len);
 		if ((len & 3) == 2) {
 			p[foc->len] = TCPOPT_NOP;
 			p[foc->len + 1] = TCPOPT_NOP;
@@ -1016,7 +1017,7 @@ static int tcp_transmit_skb(struct sock *sk, struct sk_buff *skb, int clone_it,
 
 	inet = inet_sk(sk);
 	tcb = TCP_SKB_CB(skb);
-	memset(&opts, 0, sizeof(opts));
+	bzero_fast(&opts, sizeof(opts));
 
 	if (unlikely(tcb->tcp_flags & TCPHDR_SYN))
 		tcp_options_size = tcp_syn_options(sk, skb, &opts, &md5);
@@ -1119,8 +1120,8 @@ static int tcp_transmit_skb(struct sock *sk, struct sk_buff *skb, int clone_it,
 	skb->tstamp = 0;
 
 	/* Cleanup our debris for IP stacks */
-	memset(skb->cb, 0, max(sizeof(struct inet_skb_parm),
-			       sizeof(struct inet6_skb_parm)));
+	bzero_fast(skb->cb, max(sizeof(struct inet_skb_parm),
+			        sizeof(struct inet6_skb_parm)));
 
 	err = icsk->icsk_af_ops->queue_xmit(sk, skb, &inet->cork.fl);
 
@@ -1140,7 +1141,7 @@ static int tcp_transmit_skb(struct sock *sk, struct sk_buff *skb, int clone_it,
  * NOTE: probe0 timer is not checked, do not forget tcp_push_pending_frames,
  * otherwise socket can stall.
  */
-static void tcp_queue_skb(struct sock *sk, struct sk_buff *skb)
+void tcp_queue_skb(struct sock *sk, struct sk_buff *skb)
 {
 	struct tcp_sock *tp = tcp_sk(sk);
 
@@ -1151,6 +1152,7 @@ static void tcp_queue_skb(struct sock *sk, struct sk_buff *skb)
 	sk->sk_wmem_queued += skb->truesize;
 	sk_mem_charge(sk, skb->truesize);
 }
+EXPORT_SYMBOL(tcp_queue_skb);
 
 /* Initialize TSO segments for a packet. */
 static void tcp_set_skb_tso_segs(struct sk_buff *skb, unsigned int mss_now)
@@ -1560,6 +1562,7 @@ unsigned int tcp_current_mss(struct sock *sk)
 
 	return mss_now;
 }
+EXPORT_SYMBOL(tcp_current_mss);
 
 /* RFC2861, slow part. Adjust cwnd, after it was not full during one rto.
  * As additional protections, we do not touch cwnd in retransmission phases,
@@ -2518,6 +2521,7 @@ void __tcp_push_pending_frames(struct sock *sk, unsigned int cur_mss,
 			   sk_gfp_mask(sk, GFP_ATOMIC)))
 		tcp_check_probe_timer(sk);
 }
+EXPORT_SYMBOL(__tcp_push_pending_frames);
 
 /* Send _single_ skb sitting at the send head. This function requires
  * true push pending frames to setup probe timer etc.
@@ -2839,9 +2843,19 @@ int __tcp_retransmit_skb(struct sock *sk, struct sk_buff *skb, int segs)
 		if (tcp_fragment(sk, skb, len, cur_mss, GFP_ATOMIC))
 			return -ENOMEM; /* We'll try again later. */
 	} else {
+		int delta_truesize = skb->truesize;
+
 		if (skb_unclone(skb, GFP_ATOMIC))
 			return -ENOMEM;
 
+		delta_truesize -= skb->truesize;
+		sk->sk_wmem_queued -= delta_truesize;
+		if (delta_truesize > 0) {
+			sk_mem_uncharge(sk, delta_truesize);
+			sock_set_flag(sk, SOCK_QUEUE_SHRUNK);
+		} else {
+			sk_mem_charge(sk, -delta_truesize);
+		}
 		diff = tcp_skb_pcount(skb);
 		tcp_set_skb_tso_segs(skb, cur_mss);
 		diff -= tcp_skb_pcount(skb);
@@ -3129,6 +3143,7 @@ int tcp_send_synack(struct sock *sk)
 	}
 	return tcp_transmit_skb(sk, skb, 1, GFP_ATOMIC);
 }
+EXPORT_SYMBOL(tcp_send_active_reset);
 
 /**
  * tcp_make_synack - Prepare a SYN-ACK.
@@ -3182,7 +3197,7 @@ struct sk_buff *tcp_make_synack(const struct sock *sk, struct dst_entry *dst,
 
 	mss = tcp_mss_clamp(tp, dst_metric_advmss(dst));
 
-	memset(&opts, 0, sizeof(opts));
+	bzero_fast(&opts, sizeof(opts));
 #ifdef CONFIG_SYN_COOKIES
 	if (unlikely(req->cookie_ts))
 		skb->skb_mstamp = cookie_init_timestamp(req);
@@ -3202,7 +3217,7 @@ struct sk_buff *tcp_make_synack(const struct sock *sk, struct dst_entry *dst,
 	skb_reset_transport_header(skb);
 
 	th = (struct tcphdr *)skb->data;
-	memset(th, 0, sizeof(struct tcphdr));
+	bzero_fast(th, sizeof(struct tcphdr));
 	th->syn = 1;
 	th->ack = 1;
 	tcp_ecn_make_synack(req, th);
@@ -3379,7 +3394,7 @@ static int tcp_send_syn_data(struct sock *sk, struct sk_buff *syn)
 	if (!syn_data)
 		goto fallback;
 	syn_data->ip_summed = CHECKSUM_PARTIAL;
-	memcpy(syn_data->cb, syn->cb, sizeof(syn->cb));
+	memcpy_fast(syn_data->cb, syn->cb, sizeof(syn->cb));
 	if (space) {
 		int copied = copy_from_iter(skb_put(syn_data, space), space,
 					    &fo->data->msg_iter);
diff --git a/net/ipv4/tcp_probe.c b/net/ipv4/tcp_probe.c
index 697f4c67..e431f9db 100644
--- a/net/ipv4/tcp_probe.c
+++ b/net/ipv4/tcp_probe.c
@@ -130,8 +130,8 @@ static void jtcp_rcv_established(struct sock *sk, struct sk_buff *skb,
 				tcp_probe_copy_fl_to_si4(inet, p->dst.v4, d);
 				break;
 			case AF_INET6:
-				memset(&p->src.v6, 0, sizeof(p->src.v6));
-				memset(&p->dst.v6, 0, sizeof(p->dst.v6));
+				bzero_fast(&p->src.v6, sizeof(p->src.v6));
+				bzero_fast(&p->dst.v6, sizeof(p->dst.v6));
 #if IS_ENABLED(CONFIG_IPV6)
 				p->src.v6.sin6_family = AF_INET6;
 				p->src.v6.sin6_port = inet->inet_sport;
diff --git a/net/ipv6/af_inet6.c b/net/ipv6/af_inet6.c
index 9ccbf74d..f3bd75f7 100644
--- a/net/ipv6/af_inet6.c
+++ b/net/ipv6/af_inet6.c
@@ -684,7 +684,7 @@ int inet6_sk_rebuild_header(struct sock *sk)
 		struct in6_addr *final_p, final;
 		struct flowi6 fl6;
 
-		memset(&fl6, 0, sizeof(fl6));
+		bzero_fast(&fl6, sizeof(fl6));
 		fl6.flowi6_proto = sk->sk_protocol;
 		fl6.daddr = sk->sk_v6_daddr;
 		fl6.saddr = np->saddr;
diff --git a/net/ipv6/ip6_input.c b/net/ipv6/ip6_input.c
index 9ee208a3..40ea502e 100644
--- a/net/ipv6/ip6_input.c
+++ b/net/ipv6/ip6_input.c
@@ -95,7 +95,7 @@ int ipv6_rcv(struct sk_buff *skb, struct net_device *dev, struct packet_type *pt
 		goto drop;
 	}
 
-	memset(IP6CB(skb), 0, sizeof(struct inet6_skb_parm));
+	bzero_fast(IP6CB(skb), sizeof(struct inet6_skb_parm));
 
 	/*
 	 * Store incoming device index. When the packet will
diff --git a/net/ipv6/ip6_output.c b/net/ipv6/ip6_output.c
index 3763dc01..fa157459 100644
--- a/net/ipv6/ip6_output.c
+++ b/net/ipv6/ip6_output.c
@@ -691,7 +691,7 @@ int ip6_fragment(struct net *net, struct sock *sk, struct sk_buff *skb,
 		fh = __skb_push(skb, sizeof(struct frag_hdr));
 		__skb_push(skb, hlen);
 		skb_reset_network_header(skb);
-		memcpy(skb_network_header(skb), tmp_hdr, hlen);
+		memcpy_fast(skb_network_header(skb), tmp_hdr, hlen);
 
 		fh->nexthdr = nexthdr;
 		fh->reserved = 0;
@@ -713,7 +713,7 @@ int ip6_fragment(struct net *net, struct sock *sk, struct sk_buff *skb,
 				fh = __skb_push(frag, sizeof(struct frag_hdr));
 				__skb_push(frag, hlen);
 				skb_reset_network_header(frag);
-				memcpy(skb_network_header(frag), tmp_hdr,
+				memcpy_fast(skb_network_header(frag), tmp_hdr,
 				       hlen);
 				offset += skb->len - hlen - sizeof(struct frag_hdr);
 				fh->nexthdr = nexthdr;
@@ -1020,8 +1020,8 @@ static int ip6_dst_lookup_tail(struct net *net, const struct sock *sk,
 			 * default router instead
 			 */
 			dst_release(*dst);
-			memcpy(&fl_gw6, fl6, sizeof(struct flowi6));
-			memset(&fl_gw6.daddr, 0, sizeof(struct in6_addr));
+			memcpy_fast(&fl_gw6, fl6, sizeof(struct flowi6));
+			bzero_fast(&fl_gw6.daddr, sizeof(struct in6_addr));
 			*dst = ip6_route_output(net, sk, &fl_gw6);
 			err = (*dst)->error;
 			if (err)
@@ -1582,7 +1582,7 @@ static void ip6_cork_release(struct inet_cork_full *cork,
 		cork->base.dst = NULL;
 		cork->base.flags &= ~IPCORK_ALLFRAG;
 	}
-	memset(&cork->fl, 0, sizeof(cork->fl));
+	bzero_fast(&cork->fl, sizeof(cork->fl));
 }
 
 struct sk_buff *__ip6_make_skb(struct sock *sk,
diff --git a/net/ipv6/syncookies.c b/net/ipv6/syncookies.c
index 4e7817ab..f5933705 100644
--- a/net/ipv6/syncookies.c
+++ b/net/ipv6/syncookies.c
@@ -161,7 +161,7 @@ struct sock *cookie_v6_check(struct sock *sk, struct sk_buff *skb)
 	__NET_INC_STATS(sock_net(sk), LINUX_MIB_SYNCOOKIESRECV);
 
 	/* check for timestamp cookie support */
-	memset(&tcp_opt, 0, sizeof(tcp_opt));
+	bzero_fast(&tcp_opt, sizeof(tcp_opt));
 	tcp_parse_options(sock_net(sk), skb, &tcp_opt, 0, NULL);
 
 	if (tcp_opt.saw_tstamp && tcp_opt.rcv_tsecr) {
@@ -226,7 +226,7 @@ struct sock *cookie_v6_check(struct sock *sk, struct sk_buff *skb)
 	{
 		struct in6_addr *final_p, final;
 		struct flowi6 fl6;
-		memset(&fl6, 0, sizeof(fl6));
+		bzero_fast(&fl6, sizeof(fl6));
 		fl6.flowi6_proto = IPPROTO_TCP;
 		fl6.daddr = ireq->ir_v6_rmt_addr;
 		final_p = fl6_update_dst(&fl6, rcu_dereference(np->opt), &final);
diff --git a/net/ipv6/tcp_ipv6.c b/net/ipv6/tcp_ipv6.c
index 237cc618..9852831d 100644
--- a/net/ipv6/tcp_ipv6.c
+++ b/net/ipv6/tcp_ipv6.c
@@ -68,6 +68,7 @@
 
 #include <crypto/hash.h>
 #include <linux/scatterlist.h>
+#include <linux/tempesta.h>
 
 static void	tcp_v6_send_reset(const struct sock *sk, struct sk_buff *skb);
 static void	tcp_v6_reqsk_send_ack(const struct sock *sk, struct sk_buff *skb,
@@ -137,7 +138,7 @@ static int tcp_v6_connect(struct sock *sk, struct sockaddr *uaddr,
 	if (usin->sin6_family != AF_INET6)
 		return -EAFNOSUPPORT;
 
-	memset(&fl6, 0, sizeof(fl6));
+	bzero_fast(&fl6, sizeof(fl6));
 
 	if (np->sndflow) {
 		fl6.flowlabel = usin->sin6_flowinfo&IPV6_FLOWINFO_MASK;
@@ -579,7 +580,7 @@ static int tcp_v6_md5_hash_headers(struct tcp_md5sig_pool *hp,
 	bp->len = cpu_to_be32(nbytes);
 
 	_th = (struct tcphdr *)(bp + 1);
-	memcpy(_th, th, sizeof(*th));
+	memcpy_fast(_th, th, sizeof(*th));
 	_th->check = 0;
 
 	sg_init_one(&sg, bp, sizeof(*bp) + sizeof(*th));
@@ -616,7 +617,7 @@ static int tcp_v6_md5_hash_hdr(char *md5_hash, const struct tcp_md5sig_key *key,
 clear_hash:
 	tcp_put_md5sig_pool();
 clear_hash_noput:
-	memset(md5_hash, 0, 16);
+	bzero_fast(md5_hash, 16);
 	return 1;
 }
 
@@ -663,7 +664,7 @@ static int tcp_v6_md5_hash_skb(char *md5_hash,
 clear_hash:
 	tcp_put_md5sig_pool();
 clear_hash_noput:
-	memset(md5_hash, 0, 16);
+	bzero_fast(md5_hash, 16);
 	return 1;
 }
 
@@ -702,7 +703,7 @@ static bool tcp_v6_inbound_md5_hash(const struct sock *sk,
 				      hash_expected,
 				      NULL, skb);
 
-	if (genhash || memcmp(hash_location, newhash, 16) != 0) {
+	if (genhash || memcmp_fast(hash_location, newhash, 16) != 0) {
 		NET_INC_STATS(sock_net(sk), LINUX_MIB_TCPMD5FAILURE);
 		net_info_ratelimited("MD5 Hash %s for [%pI6c]:%u->[%pI6c]:%u\n",
 				     genhash ? "failed" : "mismatch",
@@ -806,7 +807,7 @@ static void tcp_v6_send_response(const struct sock *sk, struct sk_buff *skb, u32
 	skb_reset_transport_header(buff);
 
 	/* Swap the send and the receive. */
-	memset(t1, 0, sizeof(*t1));
+	bzero_fast(t1, sizeof(*t1));
 	t1->dest = th->source;
 	t1->source = th->dest;
 	t1->doff = tot_len / 4;
@@ -835,7 +836,7 @@ static void tcp_v6_send_response(const struct sock *sk, struct sk_buff *skb, u32
 	}
 #endif
 
-	memset(&fl6, 0, sizeof(fl6));
+	bzero_fast(&fl6, sizeof(fl6));
 	fl6.daddr = ipv6_hdr(skb)->saddr;
 	fl6.saddr = ipv6_hdr(skb)->daddr;
 	fl6.flowlabel = label;
@@ -928,7 +929,7 @@ static void tcp_v6_send_reset(const struct sock *sk, struct sk_buff *skb)
 			goto out;
 
 		genhash = tcp_v6_md5_hash_skb(newhash, key, NULL, skb);
-		if (genhash || memcmp(hash_location, newhash, 16) != 0)
+		if (genhash || memcmp_fast(hash_location, newhash, 16) != 0)
 			goto out;
 	}
 #endif
@@ -1067,7 +1068,7 @@ static struct sock *tcp_v6_syn_recv_sock(const struct sock *sk, struct sk_buff *
 		newnp = inet6_sk(newsk);
 		newtp = tcp_sk(newsk);
 
-		memcpy(newnp, np, sizeof(struct ipv6_pinfo));
+		memcpy_fast(newnp, np, sizeof(struct ipv6_pinfo));
 
 		newnp->saddr = newsk->sk_v6_rcv_saddr;
 
@@ -1135,7 +1136,7 @@ static struct sock *tcp_v6_syn_recv_sock(const struct sock *sk, struct sk_buff *
 	newinet = inet_sk(newsk);
 	newnp = inet6_sk(newsk);
 
-	memcpy(newnp, np, sizeof(struct ipv6_pinfo));
+	memcpy_fast(newnp, np, sizeof(struct ipv6_pinfo));
 
 	newsk->sk_v6_daddr = ireq->ir_v6_rmt_addr;
 	newnp->saddr = ireq->ir_v6_loc_addr;
@@ -1204,7 +1205,17 @@ static struct sock *tcp_v6_syn_recv_sock(const struct sock *sk, struct sk_buff *
 			       sk_gfp_mask(sk, GFP_ATOMIC));
 	}
 #endif
-
+#ifdef CONFIG_SECURITY_TEMPESTA
+	/*
+	 * We need already initialized socket addresses,
+	 * so there is no appropriate security hook.
+	 */
+	if (tempesta_new_clntsk(newsk)) {
+		inet_csk_prepare_forced_close(newsk);
+		tcp_done(newsk);
+		goto out;
+	}
+#endif
 	if (__inet_inherit_port(sk, newsk) < 0) {
 		inet_csk_prepare_forced_close(newsk);
 		tcp_done(newsk);
@@ -1892,6 +1903,7 @@ static struct tcp_seq_afinfo tcp6_seq_afinfo = {
 		.show		= tcp6_seq_show,
 	},
 };
+EXPORT_SYMBOL(tcpv6_prot);
 
 int __net_init tcp6_proc_init(struct net *net)
 {
diff --git a/net/socket.c b/net/socket.c
index 43d2f17f..9c34e86e 100644
--- a/net/socket.c
+++ b/net/socket.c
@@ -162,6 +162,12 @@ static const struct file_operations socket_file_ops = {
 static DEFINE_SPINLOCK(net_family_lock);
 static const struct net_proto_family __rcu *net_families[NPROTO] __read_mostly;
 
+const struct net_proto_family *get_proto_family(int family)
+{
+	return rcu_dereference_bh(net_families[family]);
+}
+EXPORT_SYMBOL(get_proto_family);
+
 /*
  *	Statistics counters of the socket lists
  */
diff --git a/security/Kconfig b/security/Kconfig
index 87f2a6f8..5283a8a1 100644
--- a/security/Kconfig
+++ b/security/Kconfig
@@ -223,11 +223,13 @@ source security/tomoyo/Kconfig
 source security/apparmor/Kconfig
 source security/loadpin/Kconfig
 source security/yama/Kconfig
+source security/tempesta/Kconfig
 
 source security/integrity/Kconfig
 
 choice
 	prompt "Default security module"
+	default DEFAULT_SECURITY_TEMPESTA if SECURITY_TEMPESTA
 	default DEFAULT_SECURITY_SELINUX if SECURITY_SELINUX
 	default DEFAULT_SECURITY_SMACK if SECURITY_SMACK
 	default DEFAULT_SECURITY_TOMOYO if SECURITY_TOMOYO
@@ -238,6 +240,9 @@ choice
 	  Select the security module that will be used by default if the
 	  kernel parameter security= is not specified.
 
+	config DEFAULT_SECURITY_TEMPESTA
+		bool "Tempesta FW" if SECURITY_TEMPESTA=y
+
 	config DEFAULT_SECURITY_SELINUX
 		bool "SELinux" if SECURITY_SELINUX=y
 
@@ -257,6 +262,7 @@ endchoice
 
 config DEFAULT_SECURITY
 	string
+	default "tempesta" if DEFAULT_SECURITY_TEMPESTA
 	default "selinux" if DEFAULT_SECURITY_SELINUX
 	default "smack" if DEFAULT_SECURITY_SMACK
 	default "tomoyo" if DEFAULT_SECURITY_TOMOYO
diff --git a/security/Makefile b/security/Makefile
index 4d2d3782..675343ee 100644
--- a/security/Makefile
+++ b/security/Makefile
@@ -10,6 +10,7 @@ subdir-$(CONFIG_SECURITY_TOMOYO)        += tomoyo
 subdir-$(CONFIG_SECURITY_APPARMOR)	+= apparmor
 subdir-$(CONFIG_SECURITY_YAMA)		+= yama
 subdir-$(CONFIG_SECURITY_LOADPIN)	+= loadpin
+subdir-$(CONFIG_SECURITY_TEMPESTA)	+= tempesta
 
 # always enable default capabilities
 obj-y					+= commoncap.o
@@ -25,6 +26,7 @@ obj-$(CONFIG_SECURITY_TOMOYO)		+= tomoyo/
 obj-$(CONFIG_SECURITY_APPARMOR)		+= apparmor/
 obj-$(CONFIG_SECURITY_YAMA)		+= yama/
 obj-$(CONFIG_SECURITY_LOADPIN)		+= loadpin/
+obj-$(CONFIG_SECURITY_TEMPESTA)		+= tempesta/
 obj-$(CONFIG_CGROUP_DEVICE)		+= device_cgroup.o
 
 # Object integrity file lists
diff --git a/security/security.c b/security/security.c
index 4bf0f571..0da4df06 100644
--- a/security/security.c
+++ b/security/security.c
@@ -28,6 +28,7 @@
 #include <linux/backing-dev.h>
 #include <linux/string.h>
 #include <net/flow.h>
+#include <net/sock.h>
 
 #define MAX_LSM_EVM_XATTR	2
 
@@ -1422,6 +1423,8 @@ EXPORT_SYMBOL(security_socket_getpeersec_dgram);
 
 int security_sk_alloc(struct sock *sk, int family, gfp_t priority)
 {
+	sk->sk_security = NULL;
+
 	return call_int_hook(sk_alloc_security, 0, sk, family, priority);
 }
 
diff --git a/security/tempesta/Kconfig b/security/tempesta/Kconfig
new file mode 100644
index 00000000..0fa2cb46
--- /dev/null
+++ b/security/tempesta/Kconfig
@@ -0,0 +1,14 @@
+config SECURITY_TEMPESTA
+	bool "Tempesta FW Support"
+	depends on SECURITY && NET && INET
+	select SECURITY_NETWORK
+	select RPS
+	select CRYPTO
+	select CRYPTO_HMAC
+	select CRYPTO_SHA1
+	select CRYPTO_SHA1_SSSE3
+	default y
+	help
+	  This selects Tempesta FW security module.
+	  Further information may be found at https://github.com/natsys/tempesta
+	  If you are unsure how to answer this question, answer N.
diff --git a/security/tempesta/Makefile b/security/tempesta/Makefile
new file mode 100644
index 00000000..4c439ac0
--- /dev/null
+++ b/security/tempesta/Makefile
@@ -0,0 +1,3 @@
+obj-y := tempesta.o
+
+tempesta-y := tempesta_lsm.o
diff --git a/security/tempesta/tempesta_lsm.c b/security/tempesta/tempesta_lsm.c
new file mode 100644
index 00000000..55d11c9e
--- /dev/null
+++ b/security/tempesta/tempesta_lsm.c
@@ -0,0 +1,134 @@
+/**
+ *		Tempesta FW
+ *
+ * Copyright (C) 2014 NatSys Lab. (info@natsys-lab.com).
+ * Copyright (C) 2015-2018 Tempesta Technologies, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License,
+ * or (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
+ * FOR A PARTICULAR PURPOSE.
+ * See the GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License along with
+ * this program; if not, write to the Free Software Foundation, Inc., 59
+ * Temple Place - Suite 330, Boston, MA 02111-1307, USA.
+ */
+#include <linux/ipv6.h>
+#include <linux/lsm_hooks.h>
+#include <linux/spinlock.h>
+#include <linux/tempesta.h>
+
+static TempestaOps __rcu *tempesta_ops = NULL;
+static DEFINE_SPINLOCK(tops_lock);
+
+void
+tempesta_register_ops(TempestaOps *tops)
+{
+	spin_lock(&tops_lock);
+
+	BUG_ON(tempesta_ops);
+
+	rcu_assign_pointer(tempesta_ops, tops);
+
+	spin_unlock(&tops_lock);
+}
+EXPORT_SYMBOL(tempesta_register_ops);
+
+void
+tempesta_unregister_ops(TempestaOps *tops)
+{
+	spin_lock(&tops_lock);
+
+	BUG_ON(tempesta_ops != tops);
+
+	rcu_assign_pointer(tempesta_ops, NULL);
+
+	spin_unlock(&tops_lock);
+
+	/*
+	 * tempesta_ops is called in softirq only, so if there are some users
+	 * of the structures then they are active on their CPUs.
+	 * After the below we can be sure that nobody refers @tops and we can
+	 * go forward and destroy it.
+	 */
+	synchronize_rcu();
+}
+EXPORT_SYMBOL(tempesta_unregister_ops);
+
+int
+tempesta_new_clntsk(struct sock *newsk)
+{
+	int r = 0;
+
+	TempestaOps *tops;
+
+	WARN_ON(newsk->sk_security);
+
+	rcu_read_lock();
+
+	tops = rcu_dereference(tempesta_ops);
+	if (likely(tops))
+		r = tops->sk_alloc(newsk);
+
+	rcu_read_unlock();
+
+	return r;
+}
+EXPORT_SYMBOL(tempesta_new_clntsk);
+
+static void
+tempesta_sk_free(struct sock *sk)
+{
+	TempestaOps *tops;
+
+	if (!sk->sk_security)
+		return;
+
+	rcu_read_lock();
+
+	tops = rcu_dereference(tempesta_ops);
+	if (likely(tops))
+		tops->sk_free(sk);
+
+	rcu_read_unlock();
+}
+
+static int
+tempesta_sock_tcp_rcv(struct sock *sk, struct sk_buff *skb)
+{
+	int r = 0;
+	TempestaOps *tops;
+
+	rcu_read_lock();
+
+	tops = rcu_dereference(tempesta_ops);
+	if (likely(tops)) {
+		if (skb->protocol == htons(ETH_P_IP))
+			r = tops->sock_tcp_rcv(sk, skb);
+	}
+
+	rcu_read_unlock();
+
+	return r;
+}
+
+static struct security_hook_list tempesta_hooks[] __read_mostly = {
+	LSM_HOOK_INIT(sk_free_security, tempesta_sk_free),
+	LSM_HOOK_INIT(socket_sock_rcv_skb, tempesta_sock_tcp_rcv),
+};
+
+static __init int
+tempesta_init(void)
+{
+	security_add_hooks(tempesta_hooks, ARRAY_SIZE(tempesta_hooks),
+			   "tempesta");
+
+	return 0;
+}
+
+security_initcall(tempesta_init);
